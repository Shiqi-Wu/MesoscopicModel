\documentclass[11pt,a4paper]{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{cleveref}
\usepackage{caption}
\usepackage{subcaption}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{example}[theorem]{Example}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{note}[theorem]{Note}

% cleveref names for theorem-like environments
\crefname{theorem}{theorem}{theorems}
\Crefname{theorem}{Theorem}{Theorems}
\crefname{lemma}{lemma}{lemmas}
\Crefname{lemma}{Lemma}{Lemmas}
\crefname{proposition}{proposition}{propositions}
\Crefname{proposition}{Proposition}{Propositions}
\crefname{corollary}{corollary}{corollaries}
\Crefname{corollary}{Corollary}{Corollaries}
\crefname{definition}{definition}{definitions}
\Crefname{definition}{Definition}{Definitions}
\crefname{assumption}{assumption}{assumptions}
\Crefname{assumption}{Assumption}{Assumptions}
\crefname{condition}{condition}{conditions}
\Crefname{condition}{Condition}{Conditions}
\crefname{remark}{remark}{remarks}
\Crefname{remark}{Remark}{Remarks}
\crefname{example}{example}{examples}
\Crefname{example}{Example}{Examples}
\crefname{algorithm}{algorithm}{algorithms}
\Crefname{algorithm}{Algorithm}{Algorithms}

\usepackage[backend=biber,style=numeric,sorting=nty]{biblatex}
\addbibresource{ref.bib}

\title{Mesoscopic Model: Computational Framework for Dynamics Study}
\author{Research Project}
\date{\today}

\begin{document}

\maketitle

\tableofcontents
\newpage

% \section{Background}

% \subsection{Ising Models}

% The Ising model is a fundamental statistical mechanical model that describes the behavior of magnetic systems. For a two-dimensional square lattice with nearest-neighbor interactions, the Hamiltonian is given by:

% \begin{equation}
% 	H = -J \sum_{\langle i,j \rangle} \sigma_i \sigma_j - h \sum_i \sigma_i
% \end{equation}

% where $\sigma_i \in \{-1, +1\}$ are the spin variables, $J$ is the coupling strength, $h$ is the external magnetic field, and $\langle i,j \rangle$ denotes nearest-neighbor pairs.

% \subsubsection{Critical Temperature}

% The critical temperature $T_c$ is a fundamental parameter that determines the phase transition between ordered and disordered phases. We experimentally use $T_c = 1.0$ and $T_c = 4.0$.

% \begin{equation}
% 	T_c = \frac{2J}{k_B \ln(1 + \sqrt{2})} \approx \frac{2.269J}{k_B}
% \end{equation}

% where $k_B$ is the Boltzmann constant. This critical temperature separates two distinct phases:

% \begin{itemize}
% 	\item \textbf{High temperature regime} ($T > T_c$): The system is in a \textit{disordered phase} where spins are randomly oriented, resulting in zero net magnetization. The system exhibits thermal fluctuations and short-range correlations.

% 	\item \textbf{Low temperature regime} ($T < T_c$): The system is in an \textit{ordered phase} where spins tend to align, leading to non-zero magnetization. The system exhibits long-range order and reduced thermal fluctuations.

% 	\item \textbf{Critical region} ($T \approx T_c$): The system undergoes a phase transition with critical phenomena, including diverging correlation lengths and power-law scaling behavior.
% \end{itemize}

% The temperature ratio $T/T_c$ is a key parameter that determines the physical regime of the system. In our simulations, we often work in the high-temperature regime where $T/T_c > 1$, corresponding to the disordered phase.

% \subsubsection{Units and Normalization}

% In our computational framework, we use dimensionless units where the Boltzmann constant $k_B = 1$ and the coupling strength $J = 1$. This normalization simplifies the calculations while preserving the essential physics. The temperature $T$ is therefore dimensionless, and the critical temperature becomes:

% \begin{equation}
% 	T_c = \frac{2}{\ln(1 + \sqrt{2})} \approx 2.269
% \end{equation}

% This approach is common in computational statistical physics as it eliminates the need to track physical units while maintaining the correct scaling relationships between temperature, energy, and other thermodynamic quantities.

% In this normalization, the inverse temperature $\beta = 1/T$ is directly computed from the dimensionless temperature $T$. For example, with $T = 6.808$, we have $\beta = 1/6.808 \approx 0.147$, which represents the strength of thermal fluctuations in the system.

% \subsection{Nonlocal Interactions and Kac Potentials}

% The classical Ising model only considers nearest-neighbor interactions.
% However, many physical systems exhibit \emph{nonlocal interactions} that extend beyond immediate neighbors.
% To incorporate such effects, one introduces \emph{Kac-type potentials}, which are long-range interaction kernels that decay with distance.

% A Kac potential $J_\epsilon(x,y)$ is typically defined as a smooth, radially symmetric function that depends on a parameter $\epsilon > 0$ controlling the interaction range:
% \begin{equation}
% 	J_\epsilon(x, y) = \epsilon^{-d} J\left(\frac{|x - y|}{\epsilon}\right),
% \end{equation}
% where $J(r)$ is a normalized kernel function, and $d$ is the spatial dimension. Larger values of $\epsilon$ correspond to longer-range interactions, while smaller values approach the nearest-neighbor limit.

% A widely used example of a Kac potential is the Gaussian kernel:
% \begin{equation}\label{eq:J-Gaussian}
% 	J_\epsilon(x) \;=\; \frac{1}{(2\pi\epsilon^2)^{d/2}}
% 	\exp\!\left(-\frac{|x|^2}{2\epsilon^2}\right).
% \end{equation}

% The Ising Hamiltonian with Kac interactions is given by
% \begin{equation}\label{eq:H-Kac}
% 	H(\sigma) \;=\; -\frac{1}{2}\sum_{x,y} J_\epsilon(x,y)\,\sigma(x)\sigma(y) \;-\; h \sum_{x} \sigma(x).
% \end{equation}

% \subsection{Glauber Dynamics}
% \label{sec:glauber}
% To describe the stochastic time evolution of the spin system, we adopt \emph{Glauber dynamics}.
% At each site, spins flip randomly due to thermal fluctuations, with probabilities determined by the change in the Hamiltonian.

% \paragraph{Spin-Flip Probability.}
% Let $\beta > 0$ be the inverse temperature.
% For a given spin $\sigma(x)$, the probability of flipping at site $x$ depends on the local field
% \begin{equation}\label{eq:hloc}
% 	h_{\epsilon}(x) \;=\; h + \sum_{y\neq x} J_\epsilon(x-y)\,\sigma(y).
% \end{equation}
% The corresponding flip probability can be written in several equivalent forms:
% \begin{equation}\label{eq:flip-prob}
% 	\begin{split}
% 		P\big(\sigma(x)\!\to\!-\sigma(x)\big)
% 		 & = \frac{1}{1 + \exp\!\big(\beta \Delta H_x\big)}
% 		\quad \text{(from energy difference)}                                \\
% 		 & = \frac{1}{1 + \exp\!\big(2\beta\,\sigma(x) h_{\epsilon}(x)\big)}
% 		\quad \text{(logistic form)}                                         \\
% 		 & = \frac{\exp\!\big(-\beta\sigma(x) h_{\epsilon}(x)\big)}
% 		{2\cosh\!\big(\beta h_{\epsilon}(x)\big)}
% 		\quad \text{(factorized form, cf.~Masi et al.\cite{de1994glauber})}.
% 	\end{split}
% \end{equation}
% Here, $\Delta H_x = H(\sigma^x) - H(\sigma) = 2\sigma(x)h_\epsilon(x)$ denotes the energy change when flipping the spin at $x$.

% \paragraph{Initialization by Gaussian Random Field.}
% Let the lattice be a periodic $L\times L$ grid
% \begin{equation}
% 	\Lambda=\{(x_i,y_j):\ i,j=0,1,\dots,L-1\},\qquad (x_i=i,\ y_j=j).
% \end{equation}
% We will construct a zero-mean, stationary Gaussian random field (GRF)
% $\phi:\Lambda\to\mathbb{R}$ and then convert it to spins $\sigma_{i,j}\in\{-1,+1\}$. This initialization creates spatially correlated patterns with tunable length scales and contrast.

% A stationary GRF is fully characterized by either its covariance $C(\mathbf{r})$ or its
% power spectral density (PSD) $S(\mathbf{k})$, related by the Fourier transform:
% \begin{equation}
% 	S(\mathbf{k})=\sum_{\mathbf{r}\in\mathbb{Z}^2} C(\mathbf{r})\,e^{-i\,\mathbf{k}\cdot\mathbf{r}}\!,
% 	\qquad
% 	C(\mathbf{r})=\frac{1}{(2\pi)^2}\!\int_{[-\pi,\pi]^2} S(\mathbf{k})\,e^{i\,\mathbf{k}\cdot\mathbf{r}}\,d\mathbf{k}.
% \end{equation}

% Typical isotropic choices for $S(\mathbf{k})$ include the Gaussian spectrum
% $S(\mathbf{k})=\sigma^2\exp(-\tfrac{\ell^2}{2}\|\mathbf{k}\|^2)$ with correlation length $\ell$,
% the MatÃ©rn spectrum $S(\mathbf{k})\propto (\kappa^2+\|\mathbf{k}\|^2)^{-(\nu+1)}$ with smoothness $\nu$,
% and power-law forms $S(\mathbf{k})\propto \|\mathbf{k}\|^{-\alpha}$ for $\alpha>0$. Larger $\ell$ (or smaller $\alpha$) yields larger coherent blobs, while $\sigma^2$ controls the marginal variance.

% On the $L\times L$ periodic lattice, we use discrete wavevectors
% \begin{equation}
% 	k_x(n)=\tfrac{2\pi n}{L},\quad n=0,\dots,L-1,
% 	\qquad k_y(m)=\tfrac{2\pi m}{L},\quad m=0,\dots,L-1,
% \end{equation}
% and set $\mathbf{k}_{n,m}=(k_x(n),k_y(m))$ with $S_{n,m}:=S(\mathbf{k}_{n,m})$. We then construct Fourier coefficients $\widehat{\phi}_{n,m}$ such that
% \begin{equation}
% 	\widehat{\phi}_{n,m}\sim\mathcal{CN}(0,S_{n,m}),\quad\text{independently over $(n,m)$,}
% \end{equation}
% with Hermitian symmetry $\widehat{\phi}_{(L-n)\bmod L,(L-m)\bmod L}=\overline{\widehat{\phi}_{n,m}}$ to ensure a real field. In practice we set
% \begin{equation}
% 	\widehat{\phi}_{n,m}=\sqrt{\tfrac{S_{n,m}}{2}}\,(Z^{(1)}_{n,m}+i\,Z^{(2)}_{n,m}),\quad Z^{(1)}_{n,m},Z^{(2)}_{n,m}\overset{\text{i.i.d.}}{\sim}\mathcal{N}(0,1),
% \end{equation}
% and impose conjugacy for paired frequencies. On self-conjugate modes ($n\in\{0,L/2\}$ or $m\in\{0,L/2\}$ for even $L$), we set $\widehat{\phi}_{n,m}\in\mathbb{R}$ by taking $Z^{(2)}_{n,m}=0$.

% The GRF is obtained by inverse FFT:
% \begin{equation}
% 	\phi_{i,j}=\frac{1}{L^2}\sum_{n=0}^{L-1}\sum_{m=0}^{L-1}\widehat{\phi}_{n,m}\,\exp\!\Big(i(k_x(n)x_i+k_y(m)y_j)\Big),
% 	\qquad (i,j)\in\{0,\dots,L-1\}^2.
% \end{equation}
% Then $\phi$ is a zero-mean Gaussian random field with PSD approximately $S$. The variance of $\phi$ is
% \begin{equation}
% 	\mathrm{Var}[\phi_{i,j}]=\frac{1}{L^2}\sum_{n,m} S_{n,m},
% \end{equation}
% and one can rescale $\phi$ to achieve a target variance $\sigma_\star^2$. The generated filed $\phi$ is despicted in \cref{fig:grf}.
% \begin{figure}
% 	\centering
% 	\includegraphics[width=0.8\textwidth]{fig/Gaussian_Random_Field.jpg}
% 	\caption{Example realization of a Gaussian random field.}
% 	\label{fig:grf}
% \end{figure}

% To map $\phi$ into a magnetization-like field $m\in[-1,1]$, one option is
% \begin{equation}
% 	m_{i,j}=m_0+(1-|m_0|)\,\tanh\!\Big(\tfrac{\phi_{i,j}}{\tau}\Big),
% \end{equation}
% where $m_0$ sets the mean magnetization and $\tau$ controls the nonlinearity, with smaller $\tau$ pushing values toward $\pm 1$. Alternatively, a linear clipping scheme may be used. Spins are then sampled independently with
% \begin{equation}
% 	\mathbb{P}(\sigma_{i,j}=+1)=\tfrac{1+m_{i,j}}{2},\qquad
% 	\mathbb{P}(\sigma_{i,j}=-1)=\tfrac{1-m_{i,j}}{2},
% \end{equation}
% yielding an Ising configuration $\sigma\in\{-1,+1\}^{L\times L}$.

% The parameter choices have direct impact on structure: the correlation length $\ell$ sets the typical blob size, the global mean $m_0$ fixes magnetization bias, and the field variance or $\tau$ controls contrast. The periodic construction is consistent with the lattice boundary conditions. An equivalent spatial view is to draw white noise $w_{i,j}\sim\mathcal{N}(0,1)$ and convolve with a filter whose Fourier response is $\sqrt{S(\mathbf{k})}$:
% \begin{equation}
% 	\phi \;\stackrel{d}{=}\; \mathcal{F}^{-1}\!\Big[\sqrt{S(\mathbf{k})}\,\mathcal{F}[w]\Big],
% \end{equation}
% which for Gaussian $S$ corresponds to a Gaussian blur (with periodic wrap) of white noise.

% \paragraph{Stochastic Simulation via Gillespie Algorithm.}
% The Glauber dynamics can be efficiently simulated using the Gillespie algorithm (\cref{alg:gillespie}),
% which generates statistically exact trajectories of continuous-time Markov processes.
% The procedure is:

% \begin{enumerate}
% 	\item Compute the total flip rate
% 	      \begin{equation}
% 		      R = \sum_{x} P\big(\sigma(x)\!\to\!-\sigma(x)\big).
% 	      \end{equation}
% 	\item Draw the next time increment $\Delta t$ from an exponential distribution with mean $1/R$.
% 	\item Select a site $x$ to flip with probability proportional to its individual rate.
% 	\item Update $\sigma(x)\mapsto -\sigma(x)$, recompute local fields if needed, and repeat.
% \end{enumerate}

% \begin{algorithm}[h!]
% 	\caption{Gillespie Simulation of Glauber Dynamics}
% 	\label{alg:gillespie}
% 	\begin{algorithmic}[1]
% 		\State Initialize spin configuration $\sigma$ and compute local fields $h_\epsilon(x)$.
% 		\While{simulation time $t < t_{\mathrm{end}}$}
% 		\State Compute total flip rate
% 		\[
% 			R = \sum_{x} P\big(\sigma(x)\!\to\!-\sigma(x)\big).
% 		\]
% 		\State Sample time increment $\Delta t \sim \mathrm{Exp}(R)$.
% 		\State Select site $x$ with probability
% 		\[
% 			\frac{P(\sigma(x)\!\to\!-\sigma(x))}{R}.
% 		\]
% 		\State Flip spin: $\sigma(x) \gets -\sigma(x)$.
% 		\State Update local fields $h_\epsilon(y)$ if needed.
% 		\State Advance time: $t \gets t + \Delta t$.
% 		\EndWhile
% 	\end{algorithmic}
% \end{algorithm}
% This algorithm gives an implementation of the Gillespie method for Glauber dynamics, ensuring accurate simulation of the spin system's stochastic behavior.

% \paragraph{Stochastic Simulation via $\tau$-leaping Approximation.} The Gillespie algorithm simulates one event at a time, which can be slow for large systems.
% The \emph{$\tau$-leaping} method accelerates the simulation by leaping over a fixed time step $\tau$.
% During this interval, event rates are assumed to be constant.
% The number of events at each site is then sampled from a Poisson distribution.
% This allows many events to be processed in parallel.

% \begin{algorithm}[h]
% 	\caption{$\tau$-leaping for Glauber dynamics}
% 	\label{alg:tau-leaping}
% 	\begin{algorithmic}[1]
% 		\State Initialize spins $s$, time $t=0$
% 		\While{$t < T_{\text{end}}$}
% 		\State Compute local field $h_{\text{loc}}$
% 		\State Compute flip rates $r_i = \frac{1}{1 + \exp(2 \beta h_{\text{loc},i} s_i)}$
% 		\State Choose $\tau = \hat{\tau} / \max_i r_i$
% 		\State Sample $K_i \sim \mathrm{Poisson}(r_i \tau)$
% 		\State Update spins: if $K_i$ is odd, flip $s_i$
% 		\State $t \gets t + \tau$
% 		\EndWhile
% 	\end{algorithmic}
% \end{algorithm}

% \paragraph{Comparison of Gillespie and $\tau$-leaping} We compare the two methods within a small lattice ($L=128$) to validate the $\tau$-leaping approximation.
% We use the same initial conditions and parameters (temperature $T = 1.0$, coupling strength $J = 1.0$, and external field $h = 0.0$) for both simulations, and for each method, we run 20 independent trials to account for stochastic variability.
% The results are depicted in \cref{fig:gillespie_vs_tau_leaping}.
% The top left panel shows the averaged magnetization over time, while the top right panel displays the free energy evolution. The bottom panels illustrate the error between the two methods in terms of magnetization and free energy.
% The results demonstrate the mean energy and magnetization from both methods are in good agreement, and the varience of $\tau$-leaping is slightly larger than Gillespie.
% These findings confirm that the $\tau$-leaping method is a valid approximation for simulating Glauber dynamics, providing significant computational speedup (see \cref{tab:gillespie_vs_tau_leaping}) while maintaining accuracy with a small sacrifice in variation.
% \begin{figure}
% 	\centering
% 	\includegraphics[width=0.8\textwidth]{fig/ising_L128_ell8.0_sigma1.0_tau1.0_m00.2_beta1.0_eps0.02_compare_N20.png}
% 	\caption{Comparison of Gillespie and $\tau$-leaping methods for simulating Glauber dynamics on a $128 \times 128$ lattice. The top left panel shows the averaged magnization over time and the top right panel shows the free energy evolution over time. The bottom panels show the error between the two methods in terms of magnetization and free energy.}
% 	\label{fig:gillespie_vs_tau_leaping}
% \end{figure}

% \begin{table}
% 	\centering
% 	\begin{tabular}{ll}
% 		\hline
% 		\hline
% 		Method         & Simulation Time (20 Trails) \\
% 		\hline
% 		Gillespie      & $786.51$s                   \\
% 		$\tau$-leaping & $10.79$s                    \\
% 		\hline
% 		\hline
% 	\end{tabular}
% 	\caption{Comparison of Gillespie and $\tau$-leaping methods for simulating Glauber dynamics on a $128 \times 128$ lattice. The table shows the final magnetization, free energy, and error for both methods.}
% 	\label{tab:gillespie_vs_tau_leaping}
% \end{table}

% \section{PDE}

% \subsection{Hydrodynamic Limit for Glauber-Kac}
% \label{subsec:hydro}
% Starting from the Glauber dynamics introduced in \cref{sec:glauber}, we consider the lattice in $[0,1]^d$. When the lattice size tends to infinity $L\to\infty$, and the empirical magnetization field
% \begin{equation}
% 	m^L(t,\mathbf{x}) = \frac{1}{B^d}\sum_{z\in \Lambda_B(\mathbf{x})} \sigma_t(z)
% \end{equation}
% is coarse-grained over mesoscopic blocks of size $B$, the law of large numbers implies convergence of $m^L(t,\mathbf{x})$ to a deterministic limit $m(t,\mathbf{x})$.
% In this hydrodynamic limit, the Glauber dynamics is described by the nonlinear nonlocal PDE
% \begin{equation}\label{eq:nonlocal}
% 	\partial_t m(t,\mathbf{x}) \;=\; -\,m(t,\mathbf{x})\;+\;\tanh\!\Big(\beta\, (J_\epsilon * m)(t,\mathbf{x}) + \beta h\Big),
% 	\qquad m(0,\mathbf{x})=m_0(\mathbf{x}),
% \end{equation}
% where the convolution operator is defined by
% \begin{equation}
% 	\label{eq:convolution}
% 	(J_\epsilon * m)(\mathbf{x}) \;=\; \int_{\mathbb{T}^d} J_\epsilon(\mathbf{x},\mathbf{y})\,m(\mathbf{y})\,d\mathbf{y}.
% \end{equation}

% Equation~\eqref{eq:nonlocal} is the mesoscopic limit of the Glauber--Kac dynamics:
% the microscopic randomness averages out, and the macroscopic magnetization evolves deterministically according to a nonlocal reaction term given by the Kac potential.

% \subsection{From the Nonlocal to the Local PDE}\paragraph{Step 1: Convolution expansion.}
% Assume the rescaled kernel $J_\epsilon(x) = \epsilon^{-d} J(x/\epsilon)$ with $\int_{\mathbb{R}^d} J(z)\,dz = 1$, $J$ even, and finite second moment
% \begin{equation}
% 	m_2 = \int_{\mathbb{R}^d} |z|^2 J(z)\,dz.
% \end{equation}

% Then
% \begin{equation}
% 	(J_\epsilon * m)(x) = \int_{\mathbb{R}^d} J(z)\, m(x+\epsilon z)\,dz.
% \end{equation}
% Expanding $m(x+\epsilon z)$ in Taylor series:
% \begin{equation}
% 	m(x+\epsilon z) = m(x) + \epsilon\, z\cdot \nabla m(x)
% 	+ \frac{\epsilon^2}{2}\, z^\top \nabla^2 m(x)\, z + O(\epsilon^3).
% \end{equation}

% By symmetry $\int J(z)\,z\,dz = 0$ and isotropy
% $\int J(z)\, z_i z_j\,dz = \frac{m_2}{d}\delta_{ij}$, we obtain
% \begin{equation}
% 	(J_\epsilon * m)(x) = m(x) + \frac{\epsilon^2 m_2}{2d}\,\Delta m(x) + O(\epsilon^4).
% \end{equation}
% Here, $m_2$ is the second moment of $J$.

% Define $\varepsilon_\epsilon = \frac{\epsilon^2 m_2}{2d}$, we have the local PDE approximation
% \begin{equation}
% 	\label{eq:local_tanh}
% 	\frac{\partial m}{\partial t} = -\,m + \tanh\!\Big(\beta\big(m + \varepsilon_\epsilon \Delta m + h\big)\Big).
% \end{equation}

% \paragraph{Step 2: Expansion of $\tanh$.}
% The nonlocal dynamics reads
% \begin{equation}
% 	\partial_t m = -\,m + \tanh\!\Big(\beta\big((J_\epsilon * m)(x) + h\big)\Big).
% \end{equation}
% Using Step~1,
% \begin{equation}
% 	\beta\big((J_\epsilon*m)(x) + h\big) = a + \delta + O(\epsilon^4),
% 	\quad a = \beta(m+h), \quad \delta = \beta \varepsilon_\epsilon \Delta m.
% \end{equation}
% Expanding $\tanh$ around $a$:
% \begin{equation}
% 	\tanh(a+\delta)
% 	= \tanh(a) + \text{sech}^2(a)\,\delta + O(\delta^2).
% \end{equation}
% Thus,
% \begin{equation}
% 	\tanh\!\Big(\beta((J_\epsilon*m)(x) + h)\Big)
% 	= \tanh\!\big(\beta(m+h)\big)
% 	+ \beta \varepsilon_\epsilon \text{sech}^2\!\big(\beta(m+h)\big)\,\Delta m
% 	+ O(\epsilon^4).
% \end{equation}
% For small $m,h$, expand
% \begin{equation}
% 	\begin{split}
% 		\tanh\!\big(\beta(m+h)\big)
% 		 & = \beta(m+h) - \frac{\beta^3}{3}(m+h)^3 + O(|m+h|^5), \\
% 		\text{sech}^2\!\big(\beta(m+h)\big)
% 		 & = 1 - \beta^2(m+h)^2 + O(|m+h|^4).
% 	\end{split}
% \end{equation}

% \paragraph{Resulting local PDE.}
% Keeping terms up to cubic order in $m,h$ and $O(\epsilon^2)$ in diffusion gives
% \begin{equation}
% 	\begin{split}
% 		\label{eq:local}
% 		\partial_t m
% 		 & = \kappa\, \Delta m
% 		+ (\beta - 1)\, m - \frac{\beta^3}{3}\, m^3 + \beta h
% 		- \beta^3(m^2 h + m h^2) - \frac{\beta^3}{3} h^3
% 		+ O(\epsilon^4, |m|^5, |h|^5), \\
% 		 & = \kappa\, \Delta m
% 	\end{split}
% \end{equation}
% with the effective diffusion coefficient
% \begin{equation}
% 	\kappa = \beta \varepsilon_\epsilon = \beta\, \frac{m_2}{2d}\, \epsilon^2.
% \end{equation}


% \paragraph{Allen-Cahn form.}
% As a simplified model, we consider the Allen-Cahn equation:
% \begin{equation}
% 	\label{eq:allen-cahn}
% 	\partial_t m = \kappa\, \Delta m - f(m) + \beta h,
% 	\qquad
% 	f(m) = r m + u m^3,
% \end{equation}
% with parameters
% \begin{equation}
% 	r = 1-\beta,
% 	\qquad u = \frac{\beta^3}{3}.
% \end{equation}



% \section{Experimental Settings}

% \subsection{Parameter Configuration}

% Our computational experiments are designed to systematically explore the parameter space of the Ising model and its mesoscopic approximations. The base configuration uses:

% \begin{itemize}
% 	\item \textbf{Coupling strength}: $J = 1.0$ (dimensionless units)
% 	\item \textbf{Critical temperature}: $T_c = 2.27$ (for 2D square lattice)
% 	\item \textbf{Lattice size}: $L = 1024, 2048$ with coarse-graining to $M = 128$
% 	\item \textbf{Block size}: $B = 8, 16$ (coarse-graining ratio $L/M = 8$)
% 	\item \textbf{Spatial resolution}: $\delta = 1/M = 0.0078125$
% \end{itemize}

% \subsubsection{Temperature Regimes}

% We investigate two distinct temperature regimes:

% \begin{enumerate}
% 	\item \textbf{Low temperature} ($T = 1.0$): Below critical temperature, corresponding to the ordered phase where $T/T_c \approx 0.44$
% 	\item \textbf{High temperature} ($T = 4.0$): Above critical temperature, corresponding to the disordered phase where $T/T_c \approx 1.76$
% \end{enumerate}

% \subsubsection{External Field Variations}

% The external magnetic field $h$ is varied across four values to study its effect on the system dynamics:

% \begin{itemize}
% 	\item $h = 0.0$ (no external field)
% 	      % \item $h = 0.5$ (weak external field)
% 	\item $h = 1.0$ (moderate external field)
% 	      % \item $h = 2.0$ (strong external field)
% \end{itemize}

% \subsubsection{Kernel Range Studies}

% To investigate the role of interaction range in nonlocal dynamics, we vary the kernel domain size:

% \begin{itemize}
% 	\item \textbf{Local interactions}: $\epsilon \to 0$ (nearest-neighbor only)
% 	\item \textbf{Short-range nonlocal}: $\epsilon = 2\delta$ (extended local interactions)
% 	\item \textbf{Medium-range nonlocal}: $\epsilon = 4\delta$ (moderate nonlocal interactions)
% 	\item \textbf{Long-range nonlocal}: $\epsilon = 8\delta$ (strong nonlocal interactions)
% \end{itemize}

% This systematic variation allows us to study the transition from local to nonlocal behavior and its impact on pattern formation and dynamics.

% This computational framework will provide insights into:

% \begin{itemize}
% 	\item The relationship between microscopic and macroscopic dynamics
% 	\item The validity of continuum approximations
% 	\item The role of nonlocal interactions in pattern formation
% 	\item Machine learning approaches for complex dynamics
% \end{itemize}

% \subsection{Data v.s. PDE Solutions}
% \begin{table}[h]
% 	\centering
% 	\begin{tabular}{lllll}
% 		\hline
% 		\hline
% 		\textbf{External field $h$} & \textbf{Temperature $T$} & \textbf{Kernel parameter $\epsilon$} & \textbf{Lattice size $L$} & \textbf{Results}                                  \\
% 		\hline
% 		0                           & 1                        & 0.015625                             & 1024                      & \Cref{fig:pde_comparison_h0_T1_eps0.015625}       \\
% 		0                           & 4                        & 0.015625                             & 1024                      & \Cref{fig:pde_comparison_h0_T4_eps0.015625}       \\
% 		1                           & 1                        & 0.015625                             & 1024                      & \Cref{fig:pde_comparison_h1_T1_eps0.015625}       \\
% 		1                           & 4                        & 0.015625                             & 1024                      & \Cref{fig:pde_comparison_h1_T4_eps0.015625}       \\
% 		0                           & 1                        & 0.03125                              & 1024                      & \Cref{fig:pde_comparison_h0_T1_eps0.03125}        \\
% 		0                           & 4                        & 0.03125                              & 1024                      & \Cref{fig:pde_comparison_h0_T4_eps0.03125}        \\
% 		1                           & 1                        & 0.03125                              & 1024                      & \Cref{fig:pde_comparison_h1_T1_eps0.03125}        \\
% 		1                           & 4                        & 0.03125                              & 1024                      & \Cref{fig:pde_comparison_h1_T4_eps0.03125}        \\
% 		0                           & 1                        & 0.0625                               & 1024                      & \Cref{fig:pde_comparison_h0_T1_eps0.0625}         \\
% 		0                           & 4                        & 0.0625                               & 1024                      & \Cref{fig:pde_comparison_h0_T4_eps0.0625}         \\
% 		1                           & 1                        & 0.0625                               & 1024                      & \Cref{fig:pde_comparison_h1_T1_eps0.0625}         \\
% 		1                           & 4                        & 0.0625                               & 1024                      & \Cref{fig:pde_comparison_h1_T4_eps0.0625}         \\
% 		0                           & 1                        & 0.015625                             & 2048                      & \Cref{fig:pde_comparison_h0_T1_eps0.015625_L2048} \\
% 		0                           & 4                        & 0.015625                             & 2048                      & \Cref{fig:pde_comparison_h0_T4_eps0.015625_L2048} \\
% 		1                           & 1                        & 0.015625                             & 2048                      & \Cref{fig:pde_comparison_h1_T1_eps0.015625_L2048} \\
% 		1                           & 4                        & 0.015625                             & 2048                      & \Cref{fig:pde_comparison_h1_T4_eps0.015625_L2048} \\
% 		0                           & 1                        & 0.03125                              & 2048                      & \Cref{fig:pde_comparison_h0_T1_eps0.03125_L2048}  \\
% 		0                           & 4                        & 0.03125                              & 2048                      & \Cref{fig:pde_comparison_h0_T4_eps0.03125_L2048}  \\
% 		1                           & 1                        & 0.03125                              & 2048                      & \Cref{fig:pde_comparison_h1_T1_eps0.03125_L2048}  \\
% 		1                           & 4                        & 0.03125                              & 2048                      & \Cref{fig:pde_comparison_h1_T4_eps0.03125_L2048}  \\
% 		0                           & 1                        & 0.0625                               & 2048                      & \Cref{fig:pde_comparison_h0_T1_eps0.0625_L2048}   \\
% 		0                           & 4                        & 0.0625                               & 2048                      & \Cref{fig:pde_comparison_h0_T4_eps0.0625_L2048}   \\
% 		1                           & 1                        & 0.0625                               & 2048                      & \Cref{fig:pde_comparison_h1_T1_eps0.0625_L2048}   \\
% 		1                           & 4                        & 0.0625                               & 2048                      & \Cref{fig:pde_comparison_h1_T4_eps0.0625_L2048}   \\
% 		\hline
% 		\hline
% 	\end{tabular}
% 	\caption{Comparison of original data and PDE solutions under different external field $h$, temperature $T$, and kernel parameter $\epsilon$.}
% 	\label{tab:pde_comparison}
% \end{table}

% The results are presented in \Cref{tab:pde_comparison}.
% We compare the results from microscopic simulations (\Cref{alg:tau-leaping}) with the solutions of the nonlocal PDE (\cref{eq:nonlocal}) and the local PDE with $\tanh$ form (\cref{eq:local_tanh}) and the Allen-Cahn equation (\cref{eq:allen-cahn}) from $t=0$ to $t=20$, under different settings of external field $h$, temperature $T$, kernel parameter $\epsilon$ and lattice size $L$.
% The figures are arranged as a $4\times 4$ grid, displaying the initial state, the final state, the difference between the microscopic simulation and the the PDE solutions and macroscopic statistics (magnetization, free energy and susceptibility) over time. These metrics are computed from the coarse-grained magnetization field $m(t,x)$ and their definitions are provided below.


% % with epsilon = 0.015625

% \begin{figure}[!h]
% 	\centering
% 	\includegraphics[width=1.0\textwidth]{fig/compare_pde_solvers_L1024_h0_T1_eps0.015625.png}
% 	\caption{Comparison of original data and PDE solutions for $h=0$, $T=1$, $\epsilon=0.015625$, $L=1024$.}
% 	\label{fig:pde_comparison_h0_T1_eps0.015625}
% \end{figure}


% \begin{figure}[!h]
% 	\centering
% 	\includegraphics[width=1.0\textwidth]{fig/compare_pde_solvers_L1024_h0_T4_eps0.015625.png}
% 	\caption{Comparison of original data and PDE solutions for $h=0$, $T=4$, $\epsilon=0.015625$, $L=1024$.}
% 	\label{fig:pde_comparison_h0_T4_eps0.015625}
% \end{figure}


% \begin{figure}[!h]
% 	\centering
% 	\includegraphics[width=1.0\textwidth]{fig/compare_pde_solvers_L1024_h1_T1_eps0.015625.png}
% 	\caption{Comparison of original data and PDE solutions for $h=1$, $T=1$, $\epsilon=0.015625$, $L=1024$.}
% 	\label{fig:pde_comparison_h1_T1_eps0.015625}
% \end{figure}


% \begin{figure}[!h]
% 	\centering
% 	\includegraphics[width=1.0\textwidth]{fig/compare_pde_solvers_L1024_h1_T4_eps0.015625.png}
% 	\caption{Comparison of original data and PDE solutions for $h=1$, $T=4$, $\epsilon=0.015625$, $L=1024$.}
% 	\label{fig:pde_comparison_h1_T4_eps0.015625}
% \end{figure}

% % with epsilon = 0.03125

% \begin{figure}[!h]
% 	\centering
% 	\includegraphics[width=1.0\textwidth]{fig/compare_pde_solvers_L1024_h0_T1_eps0.03125.png}
% 	\caption{Comparison of original data and PDE solutions for $h=0$, $T=1$, $\epsilon=0.03125$, $L=1024$.}
% 	\label{fig:pde_comparison_h0_T1_eps0.03125}
% \end{figure}


% \begin{figure}[h]
% 	\centering
% 	\includegraphics[width=1.0\textwidth]{fig/compare_pde_solvers_L1024_h0_T4_eps0.03125.png}
% 	\caption{Comparison of original data and PDE solutions for $h=0$, $T=4$, $\epsilon=0.03125$, $L=1024$.}
% 	\label{fig:pde_comparison_h0_T4_eps0.03125}
% \end{figure}


% \begin{figure}[!h]
% 	\centering
% 	\includegraphics[width=1.0\textwidth]{fig/compare_pde_solvers_L1024_h1_T1_eps0.03125.png}
% 	\caption{Comparison of original data and PDE solutions for $h=1$, $T=1$, $\epsilon=0.03125$, $L=1024$.}
% 	\label{fig:pde_comparison_h1_T1_eps0.03125}
% \end{figure}


% \begin{figure}[h]
% 	\centering
% 	\includegraphics[width=1.0\textwidth]{fig/compare_pde_solvers_L1024_h1_T4_eps0.03125.png}
% 	\caption{Comparison of original data and PDE solutions for $h=1$, $T=4$, $\epsilon=0.03125$, $L=1024$.}
% 	\label{fig:pde_comparison_h1_T4_eps0.03125}
% \end{figure}


% % with epsilon = 0.0625

% \begin{figure}[!h]
% 	\centering
% 	\includegraphics[width=1.0\textwidth]{fig/compare_pde_solvers_L1024_h0_T1_eps0.0625.png}
% 	\caption{Comparison of original data and PDE solutions for $h=0$, $T=1$, $\epsilon=0.0625$, $L=1024$.}
% 	\label{fig:pde_comparison_h0_T1_eps0.0625}
% \end{figure}

% \begin{figure}[!h]
% 	\centering
% 	\includegraphics[width=1.0\textwidth]{fig/compare_pde_solvers_L1024_h0_T4_eps0.0625.png}
% 	\caption{Comparison of original data and PDE solutions for $h=0$, $T=4$, $\epsilon=0.0625$, $L=1024$.}
% 	\label{fig:pde_comparison_h0_T4_eps0.0625}
% \end{figure}

% \begin{figure}[!h]
% 	\centering
% 	\includegraphics[width=1.0\textwidth]{fig/compare_pde_solvers_L1024_h1_T1_eps0.0625.png}
% 	\caption{Comparison of original data and PDE solutions for $h=1$, $T=1$, $\epsilon=0.0625$, $L=1024$.}
% 	\label{fig:pde_comparison_h1_T1_eps0.0625}
% \end{figure}

% \begin{figure}[!h]
% 	\centering
% 	\includegraphics[width=1.0\textwidth]{fig/compare_pde_solvers_L1024_h1_T4_eps0.0625.png}
% 	\caption{Comparison of original data and PDE solutions for $h=1$, $T=4$, $\epsilon=0.0625$, $L=1024$.}
% 	\label{fig:pde_comparison_h1_T4_eps0.0625}
% \end{figure}


% % with epsilon = 0.015625, L = 2048

% \begin{figure}[!h]
% 	\centering
% 	\includegraphics[width=1.0\textwidth]{fig/compare_pde_solvers_L2048_h0_T1_eps0.015625.png}
% 	\caption{Comparison of original data and PDE solutions for $h=0$, $T=1$, $\epsilon=0.015625$, $L=2048$.}
% 	\label{fig:pde_comparison_h0_T1_eps0.015625_L2048}
% \end{figure}


% \begin{figure}[!h]
% 	\centering
% 	\includegraphics[width=1.0\textwidth]{fig/compare_pde_solvers_L2048_h0_T4_eps0.015625.png}
% 	\caption{Comparison of original data and PDE solutions for $h=0$, $T=4$, $\epsilon=0.015625$, $L=2048$.}
% 	\label{fig:pde_comparison_h0_T4_eps0.015625_L2048}
% \end{figure}


% \begin{figure}[!h]
% 	\centering
% 	\includegraphics[width=1.0\textwidth]{fig/compare_pde_solvers_L2048_h1_T1_eps0.015625.png}
% 	\caption{Comparison of original data and PDE solutions for $h=1$, $T=1$, $\epsilon=0.015625$, $L=2048$.}
% 	\label{fig:pde_comparison_h1_T1_eps0.015625_L2048}
% \end{figure}


% \begin{figure}[!h]
% 	\centering
% 	\includegraphics[width=1.0\textwidth]{fig/compare_pde_solvers_L2048_h1_T4_eps0.015625.png}
% 	\caption{Comparison of original data and PDE solutions for $h=1$, $T=4$, $\epsilon=0.015625$, $L=2048$.}
% 	\label{fig:pde_comparison_h1_T4_eps0.015625_L2048}
% \end{figure}

% % with epsilon = 0.03125

% \begin{figure}[!h]
% 	\centering
% 	\includegraphics[width=1.0\textwidth]{fig/compare_pde_solvers_L2048_h0_T1_eps0.03125.png}
% 	\caption{Comparison of original data and PDE solutions for $h=0$, $T=1$, $\epsilon=0.03125$, $L=2048$.}
% 	\label{fig:pde_comparison_h0_T1_eps0.03125_L2048}
% \end{figure}


% \begin{figure}[h]
% 	\centering
% 	\includegraphics[width=1.0\textwidth]{fig/compare_pde_solvers_L2048_h0_T4_eps0.03125.png}
% 	\caption{Comparison of original data and PDE solutions for $h=0$, $T=4$, $\epsilon=0.03125$, $L=2048$.}
% 	\label{fig:pde_comparison_h0_T4_eps0.03125_L2048}
% \end{figure}


% \begin{figure}[!h]
% 	\centering
% 	\includegraphics[width=1.0\textwidth]{fig/compare_pde_solvers_L2048_h1_T1_eps0.03125.png}
% 	\caption{Comparison of original data and PDE solutions for $h=1$, $T=1$, $\epsilon=0.03125$, $L=2048$.}
% 	\label{fig:pde_comparison_h1_T1_eps0.03125_L2048}
% \end{figure}


% \begin{figure}[h]
% 	\centering
% 	\includegraphics[width=1.0\textwidth]{fig/compare_pde_solvers_L2048_h1_T4_eps0.03125.png}
% 	\caption{Comparison of original data and PDE solutions for $h=1$, $T=4$, $\epsilon=0.03125$, $L=2048$.}
% 	\label{fig:pde_comparison_h1_T4_eps0.03125_L2048}
% \end{figure}


% % with epsilon = 0.0625, L = 2048

% \begin{figure}[!h]
% 	\centering
% 	\includegraphics[width=1.0\textwidth]{fig/compare_pde_solvers_L2048_h0_T1_eps0.0625.png}
% 	\caption{Comparison of original data and PDE solutions for $h=0$, $T=1$, $\epsilon=0.0625$, $L=2048$.}
% 	\label{fig:pde_comparison_h0_T1_eps0.0625_L2048}
% \end{figure}


% \begin{figure}[!h]
% 	\centering
% 	\includegraphics[width=1.0\textwidth]{fig/compare_pde_solvers_L2048_h0_T4_eps0.0625.png}
% 	\caption{Comparison of original data and PDE solutions for $h=0$, $T=4$, $\epsilon=0.0625$, $L=2048$.}
% 	\label{fig:pde_comparison_h0_T4_eps0.0625_L2048}
% \end{figure}

% \begin{figure}[!h]
% 	\centering
% 	\includegraphics[width=1.0\textwidth]{fig/compare_pde_solvers_L2048_h1_T1_eps0.0625.png}
% 	\caption{Comparison of original data and PDE solutions for $h=1$, $T=1$, $\epsilon=0.0625$, $L=2048$.}
% 	\label{fig:pde_comparison_h1_T1_eps0.0625_L2048}
% \end{figure}

% \begin{figure}[!h]
% 	\centering
% 	\includegraphics[width=1.0\textwidth]{fig/compare_pde_solvers_L2048_h1_T4_eps0.0625.png}
% 	\caption{Comparison of original data and PDE solutions for $h=1$, $T=4$, $\epsilon=0.0625$, $L=2048$.}
% 	\label{fig:pde_comparison_h1_T4_eps0.0625_L2048}
% \end{figure}


% % ------------------------------------------------------------
% \subsection{Error metric and statistical indicators used in comparisons}
% \label{sec:error_and_stats}

% We summarize the precise definitions of the error curves and the three statistical indicators displayed in the comparison figures (data, local PDE, nonlocal PDE). We work on the periodic domain $\Omega=[0,1]^2$ discretized by an $M\times M$ grid with spacing $\Delta x = \Delta y = 1/M$ and $N=M^2$ grid points. Spatial integrals are approximated by Riemann sums with weight $\Delta x\,\Delta y = 1/M^2$.

% \paragraph{Time alignment.} Let $\{t^{\text{data}}_n\}_{n=0}^{T_{\text{data}}-1}$, $\{t^{\text{loc}}_n\}_{n=0}^{T_{\text{loc}}-1}$ and $\{t^{\text{non}}_n\}_{n=0}^{T_{\text{non}}-1}$ denote the sampled times for the data, local PDE, and nonlocal PDE trajectories, respectively. For plotting time series jointly, we form a common index set of length $T_\ast=\min(T_{\text{data}},T_{\text{loc}},T_{\text{non}})$ by taking linearly spaced integer indices into each sequence (nearest-neighbor subsampling). We denote the aligned fields by $m_{\text{data}}(t_i,\cdot)$, $m_{\text{loc}}(t_i,\cdot)$, and $m_{\text{non}}(t_i,\cdot)$ for $i=0,\dots,T_\ast-1$.

% \paragraph{Magnetization (spatial mean).} For any field $m(t,\cdot)$,
% \begin{equation}
% 	\overline{m}(t) \;=\; \frac{1}{|\Omega|} \int_{\Omega} m(t,\mathbf{x})\,d\mathbf{x}
% 	\;\approx\; \frac{1}{M^2} \sum_{i,j=0}^{M-1} m_{i,j}(t).
% \end{equation}
% We plot $\overline{m}_{\text{data}}(t_i)$, $\overline{m}_{\text{loc}}(t_i)$, and $\overline{m}_{\text{non}}(t_i)$ on a shared time axis.

% \paragraph{Free energy used in plots (post-processing functional).} Let $\beta>0$ be the inverse temperature, $h\in\mathbb{R}$ the external field, $J_0\in\mathbb{R}$ an interaction-strength factor, and $J$ a nonnegative, radially symmetric, \emph{sum-normalized} kernel (i.e., the discrete sum of $J$ over the periodic grid equals one). Define $p=(1+m)/2$ and $q=(1-m)/2$. The free energy density used in the plots is
% \begin{equation}
% 	f(m) \;=\; \frac{1}{\beta}\Big( p\,\ln p + q\,\ln q \Big)
% 	\; -\; \frac{J_0}{2}\, m\, (J*m)
% 	\; -\; h\, m,
% \end{equation}
% and the post-processed free energy is
% \begin{equation}
% 	F[m](t) \;=\; \int_{\Omega} f\big(m(t,\mathbf{x})\big)\,d\mathbf{x}
% 	\;\approx\; \frac{1}{M^2} \sum_{i,j=0}^{M-1} \Bigg[ \frac{1}{\beta}\Big( p_{i,j}\ln p_{i,j} + q_{i,j}\ln q_{i,j} \Big)
% 		- \frac{J_0}{2}\, m_{i,j}\, (J*m)_{i,j}
% 		- h\, m_{i,j} \Bigg].
% \end{equation}
% Here $(J*m)$ is the \emph{periodic} convolution of $m$ with $J$ on the $M\times M$ grid; in computations it is obtained by constructing $J$ in real space, normalizing to unit discrete sum, applying $\mathrm{ifftshift}$, taking its $\mathrm{fft2}$, and multiplying in Fourier space before an inverse FFT.

% \emph{Remark.} This plotted functional differs from the theoretical Lyapunov functional in \cref{sec:nonlocal_theory} by constant shifts and scaling conventions (e.g., factors of $\beta$ and $J_0$); it is chosen to match the numerical post-processing used across data/local/nonlocal trajectories.

% \paragraph{Magnetic susceptibility.} The spatial susceptibility is computed as
% \begin{equation}
% 	\chi(t) \;=\; \beta\,N\,\Big( \langle m^2(t,\cdot)\rangle_x - \langle m(t,\cdot)\rangle_x^{\,2} \Big),
% \end{equation}
% where $\langle\cdot\rangle_x$ denotes the spatial average over the grid, i.e., $\langle m\rangle_x = M^{-2}\sum m_{i,j}$ and $N=M^2$. We plot $\chi_{\text{data}}(t_i)$, $\chi_{\text{loc}}(t_i)$, and $\chi_{\text{non}}(t_i)$ on a shared time axis.

% \paragraph{Cross-resolution comparability.} In resolution studies where trajectories are available on different grids $M$, we first resample all fields onto a common \emph{reference grid} (the data grid) using \emph{periodic bilinear interpolation} before computing spatial averages and functionals (\,$\overline m(t)$, $\chi(t)$, $F[m](t)$\,) and before forming error curves. This removes spurious resolution dependence from the factor $N=M^2$ and from discretization effects, ensuring that, for example, $\chi(0)$ is directly comparable across resolutions.

% \paragraph{Energy convention in plots.} To compare data and PDE on equal footing, the energy-like panel uses the \emph{same post-processed free energy} $F[m](t)$ for both data and PDE, computed from the functional above with identical kernel normalization and parameters. When precomputed $F$ is unavailable in the data files, we recompute $F[m]$ from the stored coarse-grained fields. We intentionally avoid plotting the internal energy $E$ alone, since it omits the entropic term and can differ by constant/scaling factors; $F[m]$ provides a consistent, physically meaningful comparison across methods and resolutions.

% \paragraph{Mean absolute error curves.} The error curves shown are the \emph{spatial mean absolute error} between PDE solutions and data at aligned times:
% \begin{equation}
% 	E_{\text{loc}}(t_i) \;=\; \frac{1}{M^2} \sum_{k,\ell=0}^{M-1} \big|\, m_{\text{loc};\,k\ell}(t_i) - m_{\text{data};\,k\ell}(t_i) \,\big|,
% 	\qquad
% 	E_{\text{non}}(t_i) \;=\; \frac{1}{M^2} \sum_{k,\ell=0}^{M-1} \big|\, m_{\text{non};\,k\ell}(t_i) - m_{\text{data};\,k\ell}(t_i) \,\big|.
% \end{equation}
% These correspond to the $\textit{Mean $|\mathrm{PDE}-\mathrm{Data}|$ vs Time}$ panel.

% \subsection{Main Findings from PDE Comparisons}
% \label{subsec:main_findings}
% We summarize the main findings from the PDE comparison experiments.
% First, as the lattice size $L$ increases, the standard deviation of the magnetization decreases
% (as illustrated in the three pairs of figures:
% \cref{fig:pde_comparison_h0_T1_eps0.015625,fig:pde_comparison_h0_T1_eps0.015625_L2048},
% \cref{fig:pde_comparison_h0_T4_eps0.015625,fig:pde_comparison_h0_T4_eps0.015625_L2048},
% and \cref{fig:pde_comparison_h1_T4_eps0.015625,fig:pde_comparison_h1_T4_eps0.015625_L2048}).
% This observation is consistent with the theoretical results : as $L \to \infty$, the fluctuations of the empirical magnetization vanish and the empirical magnetization converges to the solution of the hydrodynamic limit PDE.

% Second, the local PDE with $\tanh$ form and with simplified polynomial form (Allen-Cahn) both provide good approximations to the microscopic dynamics when the kernel range $\epsilon$ is small (e.g., $\epsilon=2\delta$ and $\epsilon=4\delta$).
% However, as $\epsilon$ increases (e.g., $\epsilon=8\delta$), the nonlocal PDE consistently outperforms the local PDEs in terms of accuracy.
% This is particularly evident in the low-temperature regime ($T=1$) with zero external field ($h=0$), where the local PDEs struggle to capture the intricate domain structures formed during phase separation (as shown in \cref{fig:pde_comparison_h0_T1_eps0.0625,fig:pde_comparison_h0_T1_eps0.0625_L2048}).
% In contrast, the nonlocal PDE effectively captures these structures, leading to significantly lower error metrics.
% Also, the local PDE with $\tanh$ form generally outperforms the Allen-Cahn equation.

% Third, at low temperature ($T=1$) with a narrow kernel ($\epsilon = 2\delta$)
% (see \Cref{fig:pde_comparison_h0_T1_eps0.015625,fig:pde_comparison_h0_T1_eps0.015625_L2048}),
% the non-local PDE approximation performs worse than the local Allen-Cahn model.
% This discrepancy is most likely due to numerical errors in evaluating the discrete convolution with such a narrow kernel,
% as the data are generated on a fine lattice while the non-local PDE is solved on a coarser grid.
% As the kernel range increases, these convolution errors diminish.
% A quantitative comparison of errors for different kernel ranges is provided in \Cref{tab:convolution_error}.

% \begin{table}
% 	\centering
% 	\begin{tabular}{lllll}
% 		\hline
% 		\hline
% 		\textbf{$\epsilon$} & $1\delta$ & $2\delta$ & $4\delta$ & $8\delta$ \\
% 		\hline
% 		Convolution Error   & 0.003111  & 0.001017  & 0.000366  & 0.000135  \\
% 		\hline
% 		\hline
% 	\end{tabular}
% 	\caption{Comparison of discrete convolution error with different kernel range $\epsilon$. The convolution error is calculated as the mean absolute error between the discrete convolution on a $1024\times 1024$ lattice and the coarse-grained $128 \times 128$ lattice, averaged over 20 random magnetization fields.}
% 	\label{tab:convolution_error}
% \end{table}

% \subsection{Numerical stability and CFL considerations (local PDE with RK4)}
% \label{subsec:numerical_cfl}

% This section summarizes the numerical stability constraints observed in our resolution study for the \emph{local} PDE solvers (both the Allen--Cahn polynomial form and the local-limit tanh form), and clarifies how we ensure consistent comparisons across resolutions.

% \paragraph{Root cause.} The local PDEs include a diffusion term $\kappa\,\Delta m$. When integrated with explicit Runge--Kutta (RK) methods, the stable time step is limited by the highest resolvable spatial frequency on the grid. On a periodic $M\times M$ grid with $\Delta x=\Delta y=L/M$, a conservative characterization of the largest Laplacian eigenvalue is
% \[
% 	k_\text{max}^2 \;\approx\; \Big(\tfrac{\pi}{\Delta x}\Big)^2 + \Big(\tfrac{\pi}{\Delta y}\Big)^2\,.
% \]
% For the linear test problem $u_t=\kappa\,\Delta u$, explicit RK4 is stable provided $\Delta t\,\kappa\,k_\text{max}^2$ lies within RK4's stability region along the negative real axis. Using the known stability bound $\alpha_\text{RK4}\approx 2.785$ and a safety factor $s\in(0,1)$, we enforce the limiter
% \begin{equation}
% 	\Delta t_\text{eff} \;=\; \min\!\left( \Delta t_\text{in},\; s\,\frac{\alpha_\text{RK4}}{\kappa\,k_\text{max}^2} \right),
% 	\qquad k_\text{max}^2\approx \Big(\tfrac{\pi}{\Delta x}\Big)^2+\Big(\tfrac{\pi}{\Delta y}\Big)^2,\;\; s\approx 0.5.
% 	\label{eq:rk4_cfl}
% \end{equation}
% Because $k_\text{max}^2\propto M^2$, the admissible time step scales like $\Delta t_\text{eff}\propto 1/M^2$. Hence, merely refining the grid (e.g., from $M=128$ to $M=256$) without reducing $\Delta t$ leads to instability or severe phase/dissipation errors.

% \paragraph{Implementation in our solvers.} We enforce \eqref{eq:rk4_cfl} in the local solver at runtime:
% \begin{itemize}
% 	\item Compute $\Delta t_\text{eff}$ via \eqref{eq:rk4_cfl} using the current $\kappa$, $M$, and domain size $L$.
% 	\item Keep the \emph{physical horizon} $t_\text{end}$ fixed by rescaling the number of steps as $N_\text{steps}=\lceil t_\text{end}/\Delta t_\text{eff}\rceil$.
% 	\item Record at the same cadence in physical time by adjusting the recording frequency accordingly.
% \end{itemize}
% This preserves the comparison across resolutions while ensuring stability as $M$ increases.

% \paragraph{Consistent post-processing across resolutions.} For resolution comparisons, we always resample PDE fields onto the \emph{data grid} (via periodic bilinear interpolation) \emph{before} computing statistics (magnetization, susceptibility, and the post-processed free energy $F[m]$), as well as before forming error curves. This removes spurious $M$-dependent factors (e.g., $N=M^2$ in $\chi$) and discretization discrepancies, yielding directly comparable curves at $t=0$ and beyond.

% \paragraph{Planned results to display.} To illustrate the effect and the fix, we will show:
% \begin{enumerate}
% 	\item A baseline at $M=128$ ($\Delta x=1/128$) with input time step $\Delta t_\text{in}=10^{-2}$. This case is stable and serves as reference.
% 	\item A naive refinement to $M=256$ using the \emph{same} $\Delta t_\text{in}=10^{-2}$ (no limiter): the solution exhibits instability/visible artifacts (divergence or severe distortion), confirming the $\propto 1/M^2$ constraint.
% 	\item The corrected $M=256$ run with the RK4 diffusion limiter \eqref{eq:rk4_cfl} enabled (thus $\Delta t_\text{eff}$ reduced by a factor $\approx4$): the evolution becomes stable and compares consistently with the $M=128$ reference.
% 	\item Optionally, a further refinement to $M=512$, again with \eqref{eq:rk4_cfl}, demonstrating that results remain stable as the grid is refined when $\Delta t$ scales properly.
% \end{enumerate}
% For each setting we will show the standard 4-row comparison panel (data snapshots, error vs time, magnetization vs time, and the unified free-energy and susceptibility time series), with consistent post-processing on the data grid.

% \subsubsection{CFL time step calculation for specific parameter combinations}

% For concrete illustration, we analyze the CFL limits for the parameter sets used in our computational experiments. Using \eqref{eq:rk4_cfl} with domain size $L=1$ and typical diffusion coefficient $\kappa \approx 0.1$ estimated from the Allen-Cahn parameters:

% In both parameter regimes, the naive choice $\Delta t = 0.01$ leads to severe CFL violations at fine spatial resolutions, explaining the numerical instabilities observed when CFL enforcement is disabled. The required stable time steps are 1-2 orders of magnitude smaller than commonly assumed, making automatic CFL limiting essential for robust simulations.

% \paragraph{Actual time steps used with CFL enforcement} Our implementation uses the formula
% \[
% 	\Delta t_\text{eff} = \min\!\left( \Delta t_\text{in},\; \frac{s \times \alpha_\text{RK4}}{\kappa \times k_\text{max}^2} \right)
% \]
% with safety factor $s = 0.5$, stability constant $\alpha_\text{RK4} = 2.785$, and diffusion coefficient $\kappa = 0.5 \beta J_0 \epsilon^2 \approx 0.5 \times 1 \times 1 \times (0.0625)^2 \approx 0.00195$. For the specific cases:

% \paragraph{Case 1: $h=0$, $T=1$, $\epsilon=0.0625$}
% \begin{itemize}
% 	\item $N=128$: $\Delta t_\text{eff} = \min(0.01, 2.21 \times 10^{-3}) = 2.21 \times 10^{-3}$ (CFL-limited by factor 4.5)
% 	\item $N=256$: $\Delta t_\text{eff} = \min(0.01, 5.51 \times 10^{-4}) = 5.51 \times 10^{-4}$ (CFL-limited by factor 18.1)
% \end{itemize}

% \paragraph{Case 2: $h=1$, $T=4$, $\epsilon=0.0625$}
% \begin{itemize}
% 	\item $N=128$: $\Delta t_\text{eff} = \min(0.01, 2.21 \times 10^{-3}) = 2.21 \times 10^{-3}$ (CFL-limited by factor 4.5)
% 	\item $N=256$: $\Delta t_\text{eff} = \min(0.01, 5.51 \times 10^{-4}) = 5.51 \times 10^{-4}$ (CFL-limited by factor 18.1)
% \end{itemize}

% These effective time steps ensure numerical stability while maintaining the same physical simulation time $t_\text{end} = 20.0$ by automatically increasing the number of integration steps as needed.

% We show the CFL study for the local PDE with RK4 for $h=0$, $T=1$, $\epsilon=0.0625$ and $h=1$, $T=4$, $\epsilon=0.0625$ in \cref{fig:pde_cfl_study_h0_T1_eps0.0625} and \cref{fig:pde_cfl_study_h1_T4_eps0.0625}.

% \begin{figure}[!h]
% 	\centering
% 	\includegraphics[width=1.0\textwidth]{fig/pde_cfl_study_poly_h=0_T=1_eps=0.0625_L=1024.png}
% 	\caption{CFL study for the local PDE with RK4 for $h=0$, $T=1$, $\epsilon=0.0625$.}
% 	\label{fig:pde_cfl_study_h0_T1_eps0.0625}
% \end{figure}

% \begin{figure}[!h]
% 	\centering
% 	\includegraphics[width=1.0\textwidth]{fig/pde_cfl_study_poly_h=1_T=4_eps=0.0625_L=1024.png}
% 	\caption{CFL study for the local PDE with RK4 for $h=1$, $T=4$, $\epsilon=0.0625$.}
% 	\label{fig:pde_cfl_study_h1_T4_eps0.0625}
% \end{figure}



% \section{Network Training}
% \label{sec:network}

% \subsection{Motivation}
% The hydrodynamic limit derived in \cref{subsec:hydro} leads to the mesoscopic PDE
% \begin{equation}
% 	\partial_t m(t,x) = -m(t,x) + \tanh\!\big(\beta (J_\epsilon \ast m)(t,x) + \beta h \big),
% 	\quad m(0,x) = m_0(x).
% 	\label{eq:hydro}
% \end{equation}
% This deterministic equation provides the baseline dynamics for coarse-grained magnetization.
% However, the precise form of the interaction kernel $J_\epsilon$ and the functional dependence of
% the nonlinear force may not be directly available in practical systems.
% To bridge this gap, we explore neural-network parameterizations of the generator of the dynamics.

% \subsection{Network Parameterizations}
% We consider two levels of prior knowledge.

% \paragraph{Strong prior (physics-informed).}
% We constrain the force to have the form
% \begin{equation}
% 	F(I,m,h) = -A m + \tanh(BI + C m + D h),
% 	\label{eq:strong_prior}
% \end{equation}
% where $I=(J_\theta \ast m)(x)$ denotes the nonlocal interaction term computed via a learnable kernel $J_\theta$,
% and $A,B,C,D$ are trainable parameters.
% This corresponds exactly to the \texttt{ForceTanhNet} architecture in our implementation.

% \paragraph{Weak prior (black-box MLP).}
% We also test a more flexible model
% \begin{equation}
% 	F_\theta(I,m,h) = \text{MLP}_\theta(I,m,h),
% \end{equation}
% where $\text{MLP}_\theta:\mathbb{R}^3 \to \mathbb{R}$ is a fully connected network with hidden layers (e.g.\ 3 layers, width 128, $\tanh$ activations).
% This allows the network to discover nonlinear dependencies directly from data, at the expense of interpretability.

% \subsection{Learnable Kernels}
% The convolutional operator $I_\theta = J_\theta \ast m$ is implemented by parameterized kernels constrained to be nonnegative and sum to one.
% We investigate three families:
% \begin{itemize}
% 	\item \textbf{Softmax kernel:} a fully learnable $K\times K$ kernel with entries parameterized by logits, normalized by softmax.
% 	\item \textbf{Gaussian kernel:} isotropic Gaussian with learnable variance parameter.
% 	\item \textbf{Elliptical Gaussian kernel:} anisotropic Gaussian with learnable $(\sigma_x, \sigma_y, \theta)$.
% \end{itemize}
% These architectures ensure physical consistency (positivity, normalization) while allowing different degrees of flexibility.

% \subsection{Training Objective}
% Given observed trajectories $\{m_{\text{obs}}(t_i,\cdot)\}$ from microscopic simulations,
% we train the network by minimizing the one-step prediction error of the generator:
% \begin{equation}
% 	\min_{\theta} \sum_{m \in \mathcal{D}}
% 	\left\| L_\theta(m) - \partial_t m_{\text{obs}} \right\|_{L^2(\Omega)}^2,
% 	\label{eq:training}
% \end{equation}
% where
% \begin{equation}
% 	L_\theta(m) = F_\theta(I_\theta(m), m, h).
% \end{equation}
% The time derivatives $\partial_t m_{\text{obs}}$ are estimated from data either by finite differences or regression.

% \subsection{Implementation Details}
% \begin{itemize}
% 	\item \textbf{Input representation:} magnetization fields $m$ are coarse-grained to size $M\times M$.
% 	      At each training step, we sample random spatial coordinates $(x,y)$ and evaluate both $I_\theta(m)$ and $m(x,y)$ using periodic convolution (global evaluation) or local patch sampling.
% 	\item \textbf{Loss computation:} prediction $L_\theta(m)$ is compared against the empirical $\partial_t m$ at the same coordinates.
% 	\item \textbf{Optimization:} networks are trained with Adam optimizer, learning rate $10^{-3}$, and batch size 16.
% \end{itemize}

% \subsection{Planned Experiments}
% The following experiments are included:
% \begin{enumerate}
% 	\item \textbf{Strong vs weak prior:} compare \eqref{eq:strong_prior} with unconstrained MLPs in terms of predictive accuracy and generalization.
% 	\item \textbf{Kernel families:} test softmax, Gaussian, and elliptical Gaussian kernels for learning effective interaction structures.
% 	\item \textbf{Generalization across regimes:} train at high temperature ($T > T_c$) and evaluate at low temperature ($T < T_c$), and vice versa.
% 	\item \textbf{Comparison with PDE models:} evaluate learned generators against the original hydrodynamic PDE \eqref{eq:nonlocal} and its local AllenâCahn approximation.
% \end{enumerate}
% This systematic study will assess how inductive bias and kernel parameterization influence both accuracy and interpretability of learned mesoscopic models.


\section{Regularity and well-posedness for learned nonlocal dynamics}
\label{sec:network_regularity}
\subsection{Outline}


Recall the physical mesoscopic PDE on the periodic domain $\Omega=\mathbb{T}^d$:
\[
	\partial_t m(t,x) \;=\; -\,m(t,x)\; +\; \tanh\!\big(\beta\,((J_\epsilon * m)(t,x) + h)\big),
	\qquad m(0,\cdot)=m_0(\cdot).
\]
Here $m_0\in L^\infty(\Omega)$ with $\|m_0\|_{L^\infty}\le 1$.



We aim to learn networks to approximate the target dynamics/generator
$$
	\partial_t m = \mathcal{F}[m] = \mathcal{F}_\theta\big(\mathcal{I}_{\theta\prime}[m], m, h \big),
$$
where the interaction term is $\mathcal{I}_{\theta\prime}[m]=J_{\theta\prime}*m$; more generally $\mathcal{I}_{\theta\prime}$ can be the state-dependent operator in \S\ref{subsec:state_dependent_J}. In the following, we omit the dependence on $h$, and suppose all the dynamics are under the same external field $h$.

We now specify the priors on $(\mathcal{F}_\theta,J_{\theta\prime})$.
\begin{itemize}
	\item For nonlinear functions $\mathcal{F}_\theta$, we consider two priors:
	      \begin{itemize}
		      \item \textbf{Strong prior.} $\mathcal{F}_\theta = -A m + B \tanh(C \mathcal{I}_{\theta\prime}[m] + D)$, where $\mathcal{I}_{\theta\prime}[m]=J_{\theta\prime}*m$.
		      \item \textbf{Weak prior.} $\mathcal{F}_\theta$ is a neural network $\mathrm{MLP}_\theta(I,m)$ (we discuss some specific structures in the following). % $J_{\theta\prime}$ is learnable and may be a local convolution kernel $K_\theta(x-y)$ or a state-dependent operator $K_\theta(x,y; m)$ (normalized as in \eqref{eq:state_dep_op}).
	      \end{itemize}
	\item For kernels $J_{\theta\prime}$, we consider three types (all with entries parameterized by logits, normalized by softmax):
	      \begin{itemize}
		      \item \textbf{Local kernel.} $J_{\theta\prime}$ is a local convolution kernel $J_{\theta\prime}(x-y)$, Similar to CNN.
		      \item \textbf{Non-local kernel.} $J_{\theta\prime}$ is a non-local convolution kernel $J_{\theta\prime}(x,y)$.
		      \item \textbf{State-dependent kernel, nonlinear operator.} $J_{\theta\prime}$ is a state-dependent operator $J_{\theta\prime}(x,y; m)$. %Similar to attention.
	      \end{itemize}

\end{itemize}

\begin{remark}[Regularity: Finite-energy bound]
	Let $1\le p\le\infty$ and $\tfrac1p+\tfrac1q=1$.
	Assume the kernel satisfies the uniform $L^q$ bound
	\begin{equation}\label{eq:finite_energy_pq}
		\sup_{x\in\Omega}\|J(x,\cdot)\|_{L^q(\Omega)}<\infty .
	\end{equation}
	Then, by HÃ¶lderâs inequality,
	\[
		|(J*m)(x)|
		\le \|J(x,\cdot)\|_{L^q}\,\|m\|_{L^p}
		\quad\Rightarrow\quad
		\|J*m\|_{L^\infty}
		\le \Bigl[\sup_{x\in\Omega}\|J(x,\cdot)\|_{L^q}\Bigr]\|m\|_{L^p}.
	\]
	\textit{Specifically}, taking $p=\infty$ and $q=1$ recovers the $L^\infty$â$L^1$
	form used above:
	\[
		\|J*m\|_{L^\infty}
		\le \Bigl[\sup_{x\in\Omega}\!\int_\Omega |J(x,y)|\,dy\Bigr]\|m\|_{L^\infty}.
	\]
	If \eqref{eq:finite_energy_pq} fails, e.g., $J(x,y)=|x-y|^{-1}$ on $\Omega=\mathbb{T}$, then
	for $m\equiv1$ the integral $\int_\Omega |J(x_0,y)|\,dy$ diverges for every
	$x_0$, so $J*m$ is not even pointwise finite.
\end{remark}

\begin{remark}[Identifiability]
	We separate two distinct issues.

	\smallskip
	\noindent\textbf{(i) Normalization and scale.}
	To eliminate the trivial scaling freedom
	\((\beta,J)\mapsto (c\beta,J/c)\),
	we impose an $L^{q}$âtype normalization
	\[
		\|J_{\theta'}(x,\cdot)\|_{L^{q}(\Omega)} = 1 ,
	\]
	for some convenient $q\ge 1$, e.g., when $q=1$, we have $\int_\Omega J_{\theta'}(x,y)\,dy = 1$, which makes $J_{\theta'}*m$ an average of $m$.


	\emph{Note that} this fixes only the overall scale: it does not control
	singularities or oscillations of $J$. Hence we still require the finiteâenergy bound \eqref{eq:finite_energy_pq}
	to guarantee that $J_{\theta'}*m$ is well defined.
	For example, on $\Omega=\mathbb T$,
	\[
		J(x,y)
		:= \frac{|x-y|^{-1/2}}
		{\displaystyle \int_\Omega |x-z|^{-1/2}\,dz}
		\quad\Rightarrow\quad
		\int_\Omega J(x,y)\,dy = 1
	\]
	but
	\[
		\int_\Omega |J(x,y)|\,dy
		= \frac{\displaystyle \int_\Omega |x-y|^{-1/2}\,dy}
		{\displaystyle \int_\Omega |x-z|^{-1/2}\,dz}
		= \infty,
	\]
	so normalization alone does not prevent divergence.

	After this step the kernel is identified only
	\emph{up to a global sign}, i.e.\ we know its
	magnitude but not its orientation.

	\smallskip
	\noindent\textbf{(ii) Global sign.}
	Because $\tanh$ is odd,
	\[
		B\,\tanh(C\,\cdot)
		= (-B)\,\tanh((-C)\,\cdot),
	\]
	so $(B,C,J_{\theta'})$, $(B,-C,-J_{\theta'})$ and $(-B,-C, J_{\theta'})$
	define the same operator, simply omitting $D$.
	Normalization alone cannot break this symmetry. A simple constraint such as
	\[
		\int_\Omega J(x,y)\,dy > 0
	\]
	only fixes the global sign for identifiability and does not allow one to
	distinguish whether the underlying interaction is ferromagnetic or antiferromagnetic.

	If we do not accept that the learned pair
	represents only the equivalence class
	$\{(\mathcal F,J),(-\mathcal F,-J)\}$,
	we can impose a sign convention to fix the global orientation of $J$.
	For the strong prior this can be enforced by
	\[
		B>0 \;\text{and}\; C>0
	\]
	(singly fixing one is not enough).
	For the weak prior one may instead require a positive response of
	$\mathcal F_\theta$ to its interaction input, e.g.
	\[
		\partial_I \mathcal F_\theta(I_0,m_0) > 0
		\quad
	\]
	which similarly pins down the overall orientation of $J$
	without constraining its pointwise sign. (but I'm not sure this is a reasonable solution).



	\smallskip
	These identifiability conditions are independent of
	the regularity assumption and ensure that the learned
	dynamics are unique up to this explicitly chosen
	sign convention.
\end{remark}

\begin{remark}[General properties of the interaction kernel]
	\begin{itemize}
		\item \textbf{Sign.}
		      The kernel $J$ may take positive, negative, or mixed values,
		      depending on the physical system:
		      predominantly positive for ferromagnetic couplings,
		      predominantly negative or oscillatory for antiferromagnetic or
		      spinâglassâtype interactions.

		\item \textbf{Symmetry.}
		      Symmetry $J(x,y)=J(y,x)$ is natural when interactions are pairwise
		      and the Hamiltonian depends only on $\tfrac12\!\int\!J(x,y)m(x)m(y)$,
		      but it is not required in general.
		      Non-symmetric $J$ can model directed or non-reciprocal interactions.
	\end{itemize}
\end{remark}


\paragraph{Roadmap: Regularity and well-posedness.}
Given an evolution $\partial_t m = \mathcal{F}[m]$ on a chosen function space $X$ (e.g., $L^\infty(\Omega)$), the common properties one needs to establish are:

\begin{itemize}
	\item \textbf{Boundedness and invariant region:}
	      % write an operator form likr X->TX with T=T(m)
	      there exists a nonempty closed set $B\subseteq X$ such that the solution semigroup $S_t$ generated by $\mathcal{F}$ satisfies $S_t(B)\subseteq B$ for all $t\ge 0$ (forward invariance).

	\item \textbf{Well-posedness (existence/uniqueness/continuous dependence):}
	      If $\mathcal{F}:X\to X$ is locally $p$-Lipschitz in the sense of \Cref{def:locally-p-Lip} (or globally $L^p$-Lipschitz on a forward invariant set $B\subseteq X$), then the IVP admits a unique (local, respectively global on $B$) solution and the solution map $S_t$ depends Lipschitz-continuously on the initial data (with respect to $\|\cdot\|_{L^p}$). The Lipschitz continuity of $\mathcal{F}$ and $J$ also ensures that the solution map $S_t$ is continuous on $B$.

	\item \textbf{Stability/error propagation:}
	      If both vector fields are $L^p$-Lipschitz on a forward invariant set $B\subseteq X$ with constant $L$ and the generator discrepancy $\varepsilon_*:=\sup_{m\in B}\|\mathcal{L}_\theta[m]-\mathcal{L}_*[m]\|_{L^p}$ is finite, then, by GrÃ¶nwall's inequality, for all $t\in[0,T]$,
	      $\|m_\theta(t)-m_*(t)\|_{L^p} \le (\varepsilon_*/L)\,(e^{L t}-1)$.

	\item \textbf{Lyapunov/energy dissipation (if available):}
	      Exhibiting a functional that decreases monotonically along solutions provides evidence of stability and convergence. Only for analytical solutions, I am not sure about the relationship with the network.
\end{itemize}



\begin{definition}[Locally $p$-Lipschitz]\label{def:locally-p-Lip}
	Let $1\le p\le \infty$. For a multivariate functional $G: L^p(\Omega)^n \to L^p(\Omega)^n$, we say that $G$ is \emph{locally $p$-Lipschitz} if for any $u\in L^p(\Omega)^n$ there exist constants $\delta(u)>0$ and $C(u)>0$ such that
	\[
		\|u-v\|_{L^p} < \delta(u)
		\ \Longrightarrow\ \
		\|G(u)-G(v)\|_{L^{\infty}} \le C(u)\,\|u-v\|_{L^p},
		\qquad \forall v\in L^p(\Omega)^n.
	\]
\end{definition}

\begin{remark}
	For $u:\Omega\to\mathbb{R}^n$, we use the vector-valued $L^p$ norm
	\[
		\|u\|_{L^p} \;:=\; \big\|\, |u(\cdot)|_2 \,\big\|_{L^p},
	\]
	where $|\cdot|_2$ is the Euclidean norm in $\mathbb{R}^n$ (any two norms in finite dimensions are equivalent, yielding equivalent norms on $L^p(\Omega)^n$). For $p=\infty$, $\|\cdot\|_{L^{\infty}}$ is the essential supremum.
\end{remark}


\subsection{Invariant region and Design of nonlinear functions $\mathcal{F}_\theta$}
\label{subsec:designs_lipschitz}

% We study two parameterized generator forms and give easy-to-check conditions for invariant region.

\begin{condition}[Invariant interval]\label{cond:invariant}
	Suppose $\mathcal{F}(I,m)$ satisfies
	\[
		\mathcal{F}(I,1)\le 0
		\quad\text{and}\quad
		\mathcal{F}(I,-1)\ge 0
		\qquad\text{for all } I\in[-1,1].
	\]
	Then the closed set $[-1,1]$ is positively invariant.
\end{condition}


% We just need to ensure that the invariant region is a subset of $[-1,1]$, which means $F(I, 1) \le 0$ and $F(I, -1) \ge 0$ for all $I \in [-1,1]$.



\paragraph{Strong prior.} Let $ \mathcal{F}_\theta(I,m) \;=\; -A\,m + B \tanh\big( CI + D \big),\quad A>0, B>0, C>0.$

The invariant region is $[-1,1]$ if $A > B$ and $C > 0$.



\paragraph{Weak prior.} Let $ \mathcal{F}_\theta(I,m)$ be any MLP, we can add a boundary condition \ref{cond:invariant} to ensure the invariant region is $[-1,1]$.

Specifically, we may take $\mathcal{F}_\theta(I,m) = -Am + (1 - m^2) H_\theta(I,m),$ where $H_\theta$ is any MLP, $A>0$. This form is a hard constraint.

Then $\mathcal{F}(I, 1) = -A < 0$ and $\mathcal{F}(I, -1) = A > 0$, which means the invariant region is $[-1,1]$.

% \begin{proposition}[Lipschitz on the invariant cube]
% 	\label{prop:designA_lip}
% 	If $G_\theta$ is $C^1$ and its partial derivatives are bounded on $I\in[-1,1]$, $m\in[-1,1]$, then on the cube $\{|I|\le1,|m|\le1\}$
% 	\[
% 		\big|F_\theta(I_1,m_1,h)-F_\theta(I_2,m_2,h)\big|
% 		\;\le\; \Big(A + \sup(|\partial_m G_\theta|) + \sup(|\partial_I G_\theta|)\Big)\,\big(|m_1-m_2|+|I_1-I_2|\big).
% 	\]
% 	Consequently, the operator $L_\theta[m]=F_\theta(J_\theta*m,m,h)$ is globally Lipschitz on $\{\|m\|_{L^\infty}\le1\}$ with constant $\le A+\sup(|\partial_m G_\theta|)+\sup(|\partial_I G_\theta|)$, using $\|J_\theta\|_{L^1}=1$.
% \end{proposition}

% \begin{proof}[Proof (sketch)]
% 	Mean-value theorem plus bounded derivatives; convolution is an $L^p$-contraction for unit-sum kernels.
% \end{proof}

% \paragraph{Invariance (sufficient condition).} The Nagumo condition
% \[
% 	F_\theta(I,1,h)\le 0\ \text{and}\ F_\theta(I,-1,h)\ge 0\quad \forall\ I\in[-1,1],\ h\in\mathbb{R}
% \]
% is sufficient for $[-1,1]$ to be positively invariant. Two convenient realizations:
% \begin{itemize}
% 	\item Boundary-damped form: $F_\theta(I,m,h)=-A m + (1-m^2)\,H_\theta(I,m,h)$ with $A>0$. Then $F_\theta(I,\pm1,h)=\mp A$.
% 	\item Envelope bound: ensure $\sup_{|I|,|m|\le1}|G_\theta(I,m,h)|\le A-\varepsilon$ for some $\varepsilon>0$.
% \end{itemize}


\subsection{Lipschitz of $J$ and $F$}
\begin{proposition}[Local $p$--Lipschitz continuity for $J_{\theta\prime}$]\label{prop:Lip-simple}
	Let $1 \le p \le \infty$ and let $q$ be its Holder conjugate, $1/p+1/q=1$.
	For $u\in L^p(\Omega)$ define the nonlinear integral operator
	\[
		J[u](x):=\int_\Omega K(x,y;u)\,u(y)\,dy,\qquad x\in\Omega .
	\]
	Assume the kernel $K(\cdot,\cdot;u)$ is measurable and there exist
	constants $C_K>0$, $L_K>0$, and $\delta(u)>0$ such that for all $v$ with
	$\|u-v\|_{L^p}<\delta(u)$,
	\begin{equation}\label{eq:bounds}
		\text{\normalfont(A1: Boundedness) } \sup_{x\in\Omega}\|K(x,\cdot;u)\|_{L^{q}(\Omega)}\le C_K,
		\qquad
		\sup_{x\in\Omega}\|K(x,\cdot;u)\|_{L^{q'}(\Omega)}\le C_K,            \\
	\end{equation}
	\begin{equation}\label{eq:LipK}
		\text{\normalfont(A2: Lipschitz of $K$) } \sup_{x\in\Omega}\|K(x,\cdot;u)-K(x,\cdot;v)\|_{L^{q'}(\Omega)}
		\le L_K\,\|u-v\|_{L^p(\Omega)}.
	\end{equation}
	Note that, for identifiability, we may impose an $L^{q}$-normalization
	$\|K(x,\cdot;u)\|_{L^{q}}=1$ for all $x$ in space $\Omega$ for each $u$.
	Hence the first requirement in \eqref{eq:bounds} is automatically satisfied.
	However, the second bound in \eqref{eq:bounds} must still be assumed,
	because fixing the $L^{p}$ (or $L^{q}$) norm to $1$ does not in general
	control the $L^{q'}$ norm.

	Then $J:L^p(\Omega)\to L^\infty(\Omega)$ is locally $p$--Lipschitz at $u$ and
	\[
		\|J[u]-J[v]\|_{L^\infty(\Omega)}
		\;\le\; \Big(L_K\,\|u\|_{L^p(\Omega)}+C_K\Big)\,\|u-v\|_{L^p(\Omega)}
		\quad \text{whenever }\ \|u-v\|_{L^p}<\delta(u).
	\]
\end{proposition}

\begin{proof}
	Fix $x\in\Omega$ and write
	\[
		\begin{aligned}
			J[u](x)-J[v](x)
			 & =\int_\Omega K(x,y;u)\,u(y)\,dy-\int_\Omega K(x,y;v)\,v(y)\,dy            \\
			 & =\underbrace{\int_\Omega\big(K(x,y;u)-K(x,y;v)\big)\,u(y)\,dy}_{=:T_1(x)}
			+\underbrace{\int_\Omega K(x,y;v)\,\big(u(y)-v(y)\big)\,dy}_{=:T_2(x)}.
		\end{aligned}
	\]
	For $T_1$, apply HÃ¶lder with $1/p+1/q'=1$ and \eqref{eq:LipK}:
	\[
		|T_1(x)|
		\le \|K(x,\cdot;u)-K(x,\cdot;v)\|_{L^{q'}(\Omega)}\,\|u\|_{L^p(\Omega)}
		\le L_K\,\|u\|_{L^p}\,\|u-v\|_{L^p}.
	\]
	For $T_2$, apply HÃ¶lder with the same exponents and the uniform bound in
	\eqref{eq:bounds}:
	\[
		|T_2(x)|
		\le \|K(x,\cdot;v)\|_{L^{q'}(\Omega)}\,\|u-v\|_{L^p(\Omega)}
		\le C_K\,\|u-v\|_{L^p}.
	\]
	Taking the supremum over $x\in\Omega$ and adding the two estimates gives
	\[
		\|J[u]-J[v]\|_{L^\infty}
		\le \big(L_K\,\|u\|_{L^p}+C_K\big)\,\|u-v\|_{L^p},
	\]
	which proves the local $p$--Lipschitz property.
\end{proof}

\begin{remark}[Finite--energy as a standard special case]
	For $p=\infty$ (so $q=1$), the bound \eqref{eq:bounds} reduces to the
	finite--energy condition
	$\sup_{x}\int_\Omega |K(x,y;u)|\,dy<\infty$, and \eqref{eq:LipK} becomes
	$\sup_x\int_\Omega |K(x,y;u)-K(x,y;v)|\,dy\le L_K\|u-v\|_{L^\infty}$.
\end{remark}


\begin{proposition}[Local $p$--Lipschitz continuity of $\mathcal F\!\circ\! J$]\label{prop:Lip-FJ}
	Let $1\le p\le\infty$ and let $q$ be its HÃ¶lder conjugate $1/p+1/q=1$.
	For $u\in L^p(\Omega)$ define
	\[
		J[u](x):=\int_\Omega K(x,y;u)\,u(y)\,dy,\qquad
		\mathcal F[u](x):=F\big(J[u](x),\,u(x)\big),
	\]
	where $F:\mathbb R\times\mathbb R\to\mathbb R$ is a CarathÃ©odory function
	(measurable in $x$ and continuous in the other variables).

	Assume the kernel $K$ satisfies the hypotheses of
	Proposition~\ref{prop:Lip-simple} (boundedness and Lipschitz continuity)
	so that $J:L^p(\Omega)\to L^\infty(\Omega)$ is locally $p$âLipschitz.
	Suppose further that $F$ satisfies
	\begin{equation}\label{eq:F-Lip}
		\text{\normalfont(A3: Lipschitz of $F$)}\qquad
		\bigl|F(I_1,m_1)-F(I_2,m_2)\bigr|
		\le L_F\bigl(|I_1-I_2|+|m_1-m_2|\bigr)
	\end{equation}
	for some constant $L_F>0$ and all $I_i,m_i\in[-R,R]$ for some $R>0$
	that bounds $J[u](x)$ and $u(x)$ on the neighborhood under consideration.

	Then the composite mapping
	\[
		\mathcal F:L^p(\Omega)\longrightarrow L^\infty(\Omega),
		\qquad
		\mathcal F[u](x)=F(J[u](x),u(x)),
	\]
	is locally $p$âLipschitz at $u$, and whenever $\|u-v\|_{L^p}<\delta(u)$,
	\begin{equation}\label{eq:FJ-Lip}
		\|\mathcal F[u]-\mathcal F[v]\|_{L^\infty(\Omega)}
		\;\le\;
		L_F\Bigl[\,\bigl(L_K\|u\|_{L^p}+C_K\bigr)+1\Bigr]
		\,\|u-v\|_{L^p(\Omega)}.
	\end{equation}
\end{proposition}

\begin{proof}
	Let $u,v\in L^p(\Omega)$ with $\|u-v\|_{L^p}<\delta(u)$.
	For a.e.\ $x\in\Omega$,
	\[
		\bigl|\mathcal F[u](x)-\mathcal F[v](x)\bigr|
		=\bigl|F(J[u](x),u(x))-F(J[v](x),v(x))\bigr|.
	\]
	Using the Lipschitz property \eqref{eq:F-Lip} of $F$,
	\[
		\le L_F\Bigl(
		|J[u](x)-J[v](x)| + |u(x)-v(x)|
		\Bigr).
	\]
	Taking essential supremum over $x$ gives
	\[
		\|\mathcal F[u]-\mathcal F[v]\|_{L^\infty}
		\le L_F\Bigl(
		\|J[u]-J[v]\|_{L^\infty}
		+ \|u-v\|_{L^\infty}
		\Bigr).
	\]
	Because $p\ge 1$, $\|u-v\|_{L^\infty}\le \|u-v\|_{L^p}$ on finite-measure $\Omega$.
	By Proposition~\ref{prop:Lip-simple},
	\[
		\|J[u]-J[v]\|_{L^\infty}
		\le \bigl(L_K\|u\|_{L^p}+C_K\bigr)\|u-v\|_{L^p}.
	\]
	Combine the two bounds to obtain \eqref{eq:FJ-Lip}.
\end{proof}


\begin{corollary}[Lipschitz conditions for the two priors]\label{cor:priors-Lip}
	Let $J_{\theta'}:L^p(\Omega)\to L^\infty(\Omega)$ satisfy the assumptions of
	Proposition~\ref{prop:Lip-simple}, so that $u\mapsto J_{\theta'}*u$ is locally
	$p$--Lipschitz.
	Consider $\mathcal F_\theta$ of the following two forms:

	\smallskip
	\noindent
	\textbf{(P1) Strong prior.}
	\[
		\mathcal F_\theta(m)
		= -A m + B \tanh\!\bigl(C (J_{\theta'}*m)+D\bigr).
	\]
	If $A,B,C,D\in\mathbb R$ are fixed parameters, then the map
	$F(I,m)=-Am+B\tanh(C I + D)$ is globally Lipschitz on
	$\mathbb R\times\mathbb R$ because
	\[
		\bigl|\partial_I F\bigr| = |B C|\,\operatorname{sech}^2(C I + D)\le |B C|,
		\qquad
		\bigl|\partial_m F\bigr| = |A|.
	\]
	Hence the composite operator $m\mapsto\mathcal F_\theta(m)$ is locally
	$p$--Lipschitz with constant
	\[
		L_{\!F}
		\le |A| + |B C|\,\bigl(L_K\|m\|_{L^p}+C_K\bigr).
	\]

	\smallskip
	\noindent
	\textbf{(P2) Weak prior.}
	\[
		\mathcal F_\theta(m)
		= \mathrm{MLP}_\theta\!\bigl(I,m\bigr),
		\qquad I=J_{\theta'}*m.
	\]
	Assume that the neural network $\mathrm{MLP}_\theta:\mathbb R^2\to\mathbb R$
	satisfies \emph{both} of the following:
	\begin{itemize}
		\item[(a)] each activation function is globally Lipschitz
		      (e.g.\ ReLU, $\tanh$, GELU);
		\item[(b)] every weight matrix $W_\ell$ has a finite operator norm
		      $\|W_\ell\|_{\mathrm{op}}\le M_\ell$.
	\end{itemize}
	Under (a)â(b) the network is globally Lipschitz:
	there exists $L_{\mathrm{MLP}}(\theta)>0$ such that
	\[
		\bigl|\mathrm{MLP}_\theta(I_1,m_1)
		-\mathrm{MLP}_\theta(I_2,m_2)\bigr|
		\le L_{\mathrm{MLP}}(\theta)\bigl(|I_1-I_2|+|m_1-m_2|\bigr).
	\]
	Consequently $m\mapsto \mathcal F_\theta(m)$ is locally
	$p$--Lipschitz with constant
	\[
		L_{\!F}
		\le L_{\mathrm{MLP}}(\theta)\,
		\Bigl[1 + L_K\|m\|_{L^p} + C_K\Bigr].
	\]

	Consequently $m\mapsto \mathcal F_\theta(m)$ is locally
	$p$--Lipschitz with constant
	\[
		L_{\!F}
		\le L_{\mathrm{MLP}}(\theta)\,
		\Bigl[1 + L_K\|m\|_{L^p} + C_K\Bigr].
	\]

	\smallskip
	In both cases the composite map $m\mapsto\mathcal F_\theta(m)$ satisfies
	the hypotheses of Proposition~\ref{prop:Lip-FJ}, hence
	$\mathcal F_\theta:L^p(\Omega)\to L^\infty(\Omega)$
	is locally $p$--Lipschitz.

	\medskip\noindent
	\textbf{Implementation:} (a) is easy, we need to ensure (b), control $\|W_\ell\|_{\mathrm{op}}$ during training:
	\begin{itemize}
		\item[(i)] by spectral normalization, $
			      W_\ell \;\leftarrow\;
			      \frac{W_\ell}{\sigma_{\max}(W_\ell)}$.
		\item[(ii)] by weight clipping, $W_\ell \;\leftarrow\;
			      \mathrm{clip}\bigl(W_\ell,\,-c,\,c\bigr)$.
	\end{itemize}
	by spectral normalization or by projecting/ clipping weights
	to a fixed spectral norm bound after each update.

\end{corollary}



\subsection{Local well-posedness}

\begin{theorem}[PicardâLindelÃ¶f in a Banach space; see \cite{pazy2012semigroups}]\label{thm:picard}
	Let $(\mathcal Y,\|\cdot\|_{\mathcal Y})$ be a Banach space and
	$\mathcal X_\delta := C([0,\delta],\mathcal Y)$ with
	$\|u\|_{\mathcal X_\delta} := \sup_{0\le t\le \delta}\|u(t)\|_{\mathcal Y}$.
	Assume $\mathcal L:\mathcal X_\delta\to\mathcal X_\delta$ satisfies

	(Locally Lipschitz:)
	For every $R>0$ there exists $C_R>0$ such that
	\[
		\|\mathcal L[u](\cdot,s)-\mathcal L[v](\cdot,s)\|_{\mathcal Y}
		\le C_R\|u(\cdot,s)-v(\cdot,s)\|_{\mathcal Y},
		\qquad
		\forall u,v\in\mathcal X_\delta
		\text{ with }\|u\|,\|v\|\le R,
	\]
	uniformly for $s\in[0,\delta]$.

	Then for every $u_0\in\mathcal Y$ there exists $\delta_0>0$ such that the Cauchy
	problem
	\[
		\partial_t u = \mathcal L[u], \qquad u(0)=u_0
	\]
	admits a unique solution $u\in \mathcal X_{\delta_0}$.
	Moreover, if $v$ solves the same equation with initial datum $v_0$, then
	\[
		\|u-v\|_{\mathcal X_{\delta_0}}
		\le e^{C_R\delta_0}\|u_0-v_0\|_{\mathcal Y}.
	\]
\end{theorem}

\begin{corollary}[Local well-posedness of the mesoscopic PDE]\label{thm:wellposed}
	Assume the operator $\mathcal L$ in
	\[
		\partial_t m = \mathcal L[m],\qquad m(0)=m_0
	\]
	satisfies the hypotheses of Theorem~\ref{thm:picard}
	(\emph{i.e.}, $\mathcal L$ is continuous and locally Lipschitz
	on bounded subsets of $\mathcal X_\delta$ by Proposition~\ref{prop:Lip-simple}).
	Then for every $m_0\in \mathcal Y$ there exists $\delta_0>0$
	such that this Cauchy problem admits a unique solution
	\[
		m \in C([0,\delta_0],\mathcal Y),
	\]
	and the solution depends Lipschitzâcontinuously on the initial datum:
	\[
		\|m_1 - m_2\|_{\mathcal X_{\delta_0}}
		\le e^{C_R\delta_0}\,\|m_{1,0} - m_{2,0}\|_{\mathcal Y}.
	\]
\end{corollary}


\subsection{Error and stability}
\begin{theorem}[Error propagation for two vector fields]\label{thm:err}
	Let $(\mathcal Y,\|\cdot\|_{\mathcal Y})$ and $\mathcal X_\delta$
	be as in Theorem~\ref{thm:picard}, and let $B\subset\mathcal Y$
	be a closed forwardâinvariant set.
	Suppose two operators
	\(\mathcal L_\theta,\mathcal L_*:B\to\mathcal Y\) satisfy
	the same hypotheses of Corollary~\ref{thm:wellposed}
	with a common Lipschitz constant $L\ge 0$.
	Assume further that their pointwise discrepancy is finite:
	\[
		\varepsilon_*:=\sup_{m\in B}
		\|\mathcal L_\theta[m]-\mathcal L_*[m]\|_{\mathcal Y}<\infty.
	\]
	Let $m_\theta,m_*:[0,T]\to B$ be the (unique) solutions of
	\[
		\partial_t m_\theta=\mathcal L_\theta[m_\theta],\qquad
		\partial_t m_*=\mathcal L_*[m_*],
	\]
	with the same initial condition $m_\theta(0)=m_*(0)=m_0\in B$.
	Then for all $t\in[0,T]$,
	\begin{equation}\label{eq:err-prop}
		\|m_\theta(t)-m_*(t)\|_{\mathcal Y}
		\le
		\begin{cases}
			\displaystyle \dfrac{\varepsilon_*}{L}\bigl(e^{Lt}-1\bigr), & L>0, \\[1ex]
			\displaystyle \varepsilon_*\, t,                            & L=0.
		\end{cases}
	\end{equation}
\end{theorem}

\begin{proof}
	Let $e(t):=m_\theta(t)-m_*(t)$ denote the difference of the two trajectories.
	Both $m_\theta$ and $m_*$ solve their respective evolution equations,
	\[
		\partial_t m_\theta = \mathcal L_\theta[m_\theta],
		\qquad
		\partial_t m_*      = \mathcal L_*[m_*].
	\]
	Subtracting these two equations gives
	\[
		\dot e(t)
		= \mathcal L_\theta[m_\theta(t)]-\mathcal L_*[m_*(t)] .
	\]

	\medskip\noindent
	To estimate this, add and subtract the intermediate term
	$\mathcal L_\theta[m_*(t)]$:
	\[
		\dot e(t)
		= \bigl(\mathcal L_\theta[m_\theta(t)]
		-\mathcal L_\theta[m_*(t)]\bigr)
		+\bigl(\mathcal L_\theta[m_*(t)]
		-\mathcal L_*[m_*(t)]\bigr).
	\]
	The first bracket measures the sensitivity of $\mathcal L_\theta$
	to its argument, while the second measures the discrepancy
	between the two vector fields at the same point.

	\medskip\noindent
	By assumption \textnormal{(A3: Lipschitz)},
	\[
		\bigl\|\mathcal L_\theta[m_\theta(t)]
		-\mathcal L_\theta[m_*(t)]\bigr\|_{\mathcal Y}
		\le L\,\|m_\theta(t)-m_*(t)\|_{\mathcal Y}
		= L\,\|e(t)\|_{\mathcal Y}.
	\]
	By the definition of the generator discrepancy
	\(
	\varepsilon_*:=\sup_{m\in B}
	\|\mathcal L_\theta[m]-\mathcal L_*[m]\|_{\mathcal Y},
	\)
	\[
		\bigl\|\mathcal L_\theta[m_*(t)]
		-\mathcal L_*[m_*(t)]\bigr\|_{\mathcal Y}
		\le \varepsilon_*.
	\]
	Combining these two estimates yields the differential inequality
	\[
		\|e'(t)\|_{\mathcal Y}
		\le L\,\|e(t)\|_{\mathcal Y}+\varepsilon_*.
	\]

	\medskip\noindent
	Set $g(t):=\|e(t)\|_{\mathcal Y}$.  Since $g$ is absolutely continuous,
	$g'(t)\le \|e'(t)\|_{\mathcal Y}$ for almost every $t$, so
	\[
		g'(t) \le L\,g(t) + \varepsilon_*,
		\qquad g(0)=0.
	\]
	An application of GrÃ¶nwallâs inequality gives
	\[
		g(t)\le
		\begin{cases}
			\dfrac{\varepsilon_*}{L}\bigl(e^{L t}-1\bigr), & L>0, \\[1ex]
			\varepsilon_* t,                               & L=0.
		\end{cases}
	\]
	This is exactly the bound \eqref{eq:err-prop}.

	\medskip\noindent
	Finally, because $B$ is assumed forwardâinvariant,
	both trajectories $m_\theta(t)$ and $m_*(t)$ remain in $B$
	for all $t\in[0,T]$, ensuring that the Lipschitz and local boundedness
	assumptions used above remain valid throughout the time interval.
\end{proof}


\subsection{Summary of Assumptions and Priors}
\label{subsec:summary_assumptions_priors}

\begin{itemize}
	\item \textbf{Assumptions:}
	      \begin{itemize}
		      \item (A0: Identifiability): Normalized Kernel: $\|J_{\theta'}(x,\cdot)\|_{L^{p}(\Omega)}=1$. \textbf{Global sign: ?}
		      \item (A1: Boundedness of $J_{\theta'}$ in $L_q$): $\sup_{x\in\Omega}\|J_{\theta'}(x,\cdot)\|_{L^{q}(\Omega)}\le C_J$
		      \item (A2: Lipschitz of $J_{\theta'}$ in $L_q'$): $\sup_{x\in\Omega}\|J_{\theta'}(x,\cdot;u)-J_{\theta'}(x,\cdot;v)\|_{L^{q'}(\Omega)}\le C_J\|u-v\|_{L^{q'}(\Omega)}$.
		      \item (A3: Lipschitz of $\mathcal{F}_\theta$ in $L^p$): $\sup_{x\in\Omega}\|\mathcal{F}_\theta(x,u)-F_\theta(x,v)\|_{L^{p}(\Omega)}\le C_F\|u-v\|_{L^{p}(\Omega)}$.
		            %   \item (A3: Lipschitz of $\mathcal L_\theta$ and $\mathcal L_*$): there exists $L>0$ such that for all $m\in B$,
		            %         \[
		            %             \|\mathcal L_\theta[m]-\mathcal L_*[m]\|_{\mathcal Y}\le L\|m-m\|_{\mathcal Y}.
		            %         \]
		            %   \item (A4: Locally bounded): for every $R>0$ there exists $M_R>0$ such that
		            %         \[\sup_{m\in B}\|\mathcal L_\theta[m]-\mathcal L_*[m]\|_{\mathcal Y}\le M_R.\]
	      \end{itemize}
	\item \textbf{Priors on $\mathcal{F}_\theta$:}
	      \begin{itemize}
		      \item (PF1: Strong prior): $\mathcal{F}_\theta = -A m + B \tanh(C \mathcal{I}_{\theta\prime}[m] + D)$, where $\mathcal{I}_{\theta\prime}[m]=J_{\theta\prime}*m$.

		            Here, we should have $A>B>0$ and $C>0$.
		      \item (PF2: Weak prior): $\mathcal{F}_\theta$ is a neural network $\mathrm{MLP}_\theta(I,m)$ (we discuss some specific structures in the following).
	      \end{itemize}

	\item \textbf{Priors on $J_{\theta\prime}$:}
	      \begin{itemize}
		      \item (PJ1: Local kernel): $J_{\theta\prime}$ is a local convolution kernel $J_{\theta\prime}(x-y)$.
		      \item (PJ2: Non-local kernel): $J_{\theta\prime}$ is a non-local convolution kernel $J_{\theta\prime}(x,y)$.
		      \item (PJ3: State-dependent kernel, nonlinear operator): $J_{\theta\prime}$ is a state-dependent operator $J_{\theta\prime}(x,y; m)$.
	      \end{itemize}
\end{itemize}

\subsection{Discussion on Nemytskii operators}

\paragraph{Classical Nemytskii operator.}
Let $\Omega\subset\mathbb R^d$ be measurable and $1\le p<\infty$.
Given a CarathÃ©odory function $f:\Omega\times\mathbb R\to\mathbb R$
(measurable in $x$, continuous in $z$), the \emph{Nemytskii operator}
$N_f:L^p(\Omega)\to L^p(\Omega)$ is defined by
\[
	(N_f u)(x) := f\bigl(x,u(x)\bigr).
\]
A standard result (see e.g.\ \cite[Thm.~1.2.5]{amann1995linear},
\cite[Thm.~2.3.5]{pazy2012semigroups}) (TODO: check the thm) states that if
\[
	|f(x,z_1)-f(x,z_2)| \le L |z_1-z_2|
	\quad\text{for a.e. }x\in\Omega\text{ and all }z_1,z_2\in\mathbb R,
\]
and $f(\cdot,0)\in L^p(\Omega)$, then $N_f$ is globally Lipschitz on
$L^p(\Omega)$ with the same constant $L$.

\paragraph{Our composite operator.}
In our model the nonlinearity depends not only on $m(x)$
but also on the nonlocal interaction
\[
	I(x) := (J[m])(x) := \int_\Omega J(x,y)m(y)\,dy,
\]
and we consider
\[
	\mathcal F[m](x) := F\bigl(I(x),\,m(x)\bigr).
\]
Here $J:L^p(\Omega)\to L^\infty(\Omega)$ is an integral operator,
assumed to satisfy the finiteâenergy bound
\[
	\sup_{x\in\Omega}\int_\Omega |J(x,y)|\,dy < \infty,
\]
and to be locally $p$âLipschitz as in \eqref{eq:bounds}â\eqref{eq:LipK}.

\begin{theorem}[Local $p$âLipschitz continuity of $\mathcal F$]
	Let $1\le p\le\infty$ and let $J$ satisfy
	\eqref{eq:bounds}â\eqref{eq:LipK}.
	Assume that $F:\mathbb R\times\mathbb R\to\mathbb R$ is
	CarathÃ©odory and locally Lipschitz in both variables:
	for every $R>0$ there exist $L_I(R),L_m(R)$ such that
	\[
		|F(I_1,m_1)-F(I_2,m_2)|
		\le L_I(R)|I_1-I_2| + L_m(R)|m_1-m_2|
	\]
	whenever $|I_i|,|m_i|\le R$.
	Then for every bounded subset $B\subset L^p(\Omega)$
	the map $m\mapsto \mathcal F[m]$ is locally $p$âLipschitz on $B$:
	for some $C_B>0$,
	\[
		\|\mathcal F[u]-\mathcal F[v]\|_{L^p}
		\le C_B\,\|u-v\|_{L^p},
		\qquad u,v\in B.
	\]
\end{theorem}

\begin{proof}
	Let $u,v\in B$.  Write
	\[
		\mathcal F[u]-\mathcal F[v]
		= F(J[u],u)-F(J[v],v).
	\]
	By the Lipschitz property of $F$ on a range containing
	$\{(J[w](x),w(x)):w\in B\}$ we have
	\[
		|F(J[u],u)-F(J[v],v)|
		\le L_I |J[u]-J[v]| + L_m |u-v|.
	\]
	Taking $L^p$ norms and using the triangle inequality,
	\[
		\|\mathcal F[u]-\mathcal F[v]\|_{L^p}
		\le L_I \|J[u]-J[v]\|_{L^p} + L_m \|u-v\|_{L^p}.
	\]
	Since $J:L^p\to L^\infty$ is locally $p$âLipschitz by
	\eqref{eq:bounds}â\eqref{eq:LipK}, it is also locally
	$L^p\to L^p$ Lipschitz.  Hence
	\[
		\|J[u]-J[v]\|_{L^p} \le L_J \|u-v\|_{L^p}.
	\]
	Combining these estimates gives
	\[
		\|\mathcal F[u]-\mathcal F[v]\|_{L^p}
		\le \bigl(L_I L_J + L_m\bigr) \|u-v\|_{L^p},
	\]
	which proves the claim.
\end{proof}

We note that
\text{(Nemytskii conditions on $F$ and $J$) } $\Rightarrow$
\text{(Locally Lipschitz condition on $\mathcal L$, A3: \eqref{eq:F-Lip})},
\text{but not conversely.}






% \subsection{State-dependent nonlocal operator and dynamics}
% \label{subsec:state_dependent_J}

% We allow the interaction kernel to depend on the evolving field. Let $\Omega=\mathbb{T}^d$ and $u:\Omega\to\mathbb{R}$. Consider a measurable, nonnegative kernel
% \[
% 	K: \Omega\times\Omega\times\mathbb{R} \to [0,\infty),\qquad (x,y,\xi)\mapsto K(x,y;\xi),
% \]
% and define, for a given state $u$, the (pointwise) normalizer and the normalized, state-dependent nonlocal operator
% \begin{equation}
% 	Z_u(x) \;:=\; \int_{\Omega} K\big(x,y; u\big)\,dy\;>\;0,\qquad
% 	I_u[u](x) \;:=\; \frac{\displaystyle \int_{\Omega} K\big(x,y; u\big)\,u(y)\,dy}{\displaystyle Z_u(x)}.
% 	\label{eq:state_dep_op}
% \end{equation}
% If $K$ is already normalized in $y$ for each $(x,\xi)$, i.e., $\int_{\Omega}K(x,y;\xi)\,dy\equiv1$, then $I_u[u](x)=\int_{\Omega}K(x,y;u)\,u(y)\,dy$.

% Known dynamics in this work can be written using \eqref{eq:state_dep_op} as follows.

\subsection{Experimental Settings}

All kernels are normalized in $y$ for each $(x,\xi)$, i.e., $\int_{\Omega}K(x,y;\xi)\,dy\equiv1$.


\noindent
Strong prior w/ or w/o $A>B>0$ and $C>0$.

\noindent
Weak prior w/ or w/o spectral normalization or weight clipping.

\begin{itemize}
	\item (PF1, PJ1): Strong prior, local kernel.
	\item (PF1, PJ2): Strong prior, non-local kernel.
	\item (PF1, PJ3): Strong prior, state-dependent kernel.
\end{itemize}

\begin{itemize}
	\item (PF2, PJ1): Weak prior, local kernel.
	\item (PF2, PJ2): Weak prior, non-local kernel.
	\item (PF2, PJ3): Weak prior, state-dependent kernel.
\end{itemize}


Our evaluation may be related to both the error in gradient flow and the error in trajectory.

% \section{Theoretical analysis of the nonlocal Glauber-Kac PDE (GPT-generated, to be checked)} 
% \label{sec:nonlocal_theory}

% We summarize key analytical properties of the nonlocal evolution \eqref{eq:nonlocal-recall} on the periodic domain $\Omega=\mathbb{T}^d$, where the Kac kernel $J_\epsilon$ is nonnegative and normalized (so that $\int_\Omega J_\epsilon = 1$), and $\beta>0$ and $h\in\mathbb{R}$ are fixed parameters.

% \subsection{Well-posedness and invariant bounds}
% Let $\mathcal{J}[m] = (J_\epsilon * m)$ denote periodic convolution. The right-hand side
% \[
% \mathcal{F}[m] \;=\; -m + \tanh\big(\beta\,(\mathcal{J}[m] + h)\big)
% \]
% is locally Lipschitz on $L^\infty(\Omega)$ because $\tanh$ is globally Lipschitz and $\mathcal{J}$ is a bounded linear operator on $L^\infty$. Hence, for any $m_0\in L^\infty$, there exists a unique mild/strong solution $m(t)\in C([0,\infty);L^\infty)$.

% Moreover, the interval $(-1,1)$ is invariant: if $\|m_0\|_{L^\infty}\le 1$, then $\|m(t)\|_{L^\infty}<1$ for all $t>0$. Indeed, for any fixed $x$, the map $u\mapsto -u+\tanh(\beta a)$ with $a\in\mathbb{R}$ points inward at $u=\pm1$ since $-\,\operatorname{sign}(u)+\tanh(\cdot)$ has opposite sign at the boundary, and $|\tanh|<1$.

% \subsection{Spatially uniform equilibria and mean-field equation}
% Spatially uniform equilibria $m(t,x)\equiv m^*$ satisfy the scalar fixed-point equation
% \begin{equation}\label{eq:mf_fp}
%  m^* \;=\; \tanh\big(\beta(m^* + h)\big),
% \end{equation}
% which coincides with the Curie--Weiss mean-field self-consistency relation. For $h=0$, the unique solution is $m^*=0$ when $\beta\le1$, while for $\beta>1$ a pitchfork bifurcation occurs: $m^*=0$ becomes unstable and two stable nonzero equilibria $\pm m^*(\beta)$ emerge.

% \subsection{Linearization and spectral stability}
% Let $m(t,x)=m^*+\varepsilon\,\varphi(t,x)$ with $m^*$ solving \eqref{eq:mf_fp}. Using $\tanh'(z)=\operatorname{sech}^2(z)$,
% \[
% \partial_t \varphi \;=\; -\varphi\; +\; \beta\,\operatorname{sech}^2\!\big(\beta(m^*+h)\big)\; (J_\epsilon*\varphi)\; +\; O(\varepsilon^2).
% \]
% Expanding in Fourier modes $\varphi_k e^{i k\cdot x}$, with $\widehat{J_\epsilon}(k)$ the Fourier transform of $J_\epsilon$, each mode evolves as $\partial_t \varphi_k = \lambda_k\,\varphi_k$ with growth rate
% \begin{equation}\label{eq:lin_rate}
%  \lambda_k \;=\; -1\; +\; \beta\,\operatorname{sech}^2\!\big(\beta(m^*+h)\big)\,\widehat{J_\epsilon}(k).
% \end{equation}
% Because $J_\epsilon\ge0$ and is normalized, $\widehat{J_\epsilon}(0)=1$ and $\widehat{J_\epsilon}(k)\le1$. Thus the first loss of stability occurs at the homogeneous mode $k=0$; there is no finite-wavenumber (Turing-type) selection. In particular, for $h=0$ and the disordered equilibrium $m^*=0$ one has $\lambda_k = -1 + \beta\widehat{J_\epsilon}(k)$, leading to the mean-field critical value $\beta_c=1$.

% \subsection{Lyapunov functional (free energy) and dissipation}
% Define the entropy density $\Phi:[-1,1]\to\mathbb{R}$ by $\Phi'(m)=\operatorname{artanh}(m)$, e.g.
% \begin{equation}\label{eq:entropy}
%  \Phi(m) \;=\; \tfrac12\Big[(1+m)\ln(1+m)+(1-m)\ln(1-m)\Big] - \ln 2.
% \end{equation}
% Consider the functional
% \begin{equation}\label{eq:free_energy}
%  \mathcal{F}[m] \;=\; \int_\Omega \Big( \Phi(m(x)) - \tfrac{\beta}{2}\,m(x)\,(J_\epsilon*m)(x) - \beta h\,m(x) \Big)\,dx.
% \end{equation}
% Its $L^2$ variational derivative is
% \begin{equation}\label{eq:variational}
%  \frac{\delta \mathcal{F}}{\delta m}(x) \;=\; \operatorname{artanh}(m(x)) - \beta\Big((J_\epsilon*m)(x)+h\Big).
% \end{equation}
% Along solutions of \eqref{eq:nonlocal-recall},
% \begin{align}
%  \frac{d}{dt}\,\mathcal{F}[m(t)]
%  &= \int_\Omega \frac{\delta \mathcal{F}}{\delta m}(x)\,\partial_t m(t,x)\,dx \\
%  &= \int_\Omega \Big( \operatorname{artanh}(m) - \beta(\mathcal{J}[m]+h) \Big)\,\Big( -m + \tanh\big(\beta(\mathcal{J}[m]+h)\big) \Big)\,dx.
% \end{align}
% Using that $\tanh$ and $\operatorname{artanh}$ are mutual inverses and strictly increasing, one has the pointwise inequality
% \[
%  \big(\operatorname{artanh}(x) - y\big)\,\big(\tanh(y) - x\big) \;\le\; 0\quad \text{for all } x\in(-1,1),\; y\in\mathbb{R},
% \]
% with equality iff $x=\tanh(y)$. Hence $\tfrac{d}{dt}\,\mathcal{F}[m(t)] \le 0$, and equality holds precisely at equilibria satisfying $m = \tanh(\beta(\mathcal{J}[m]+h))$. Therefore, $\mathcal{F}$ is a Lyapunov functional, and solutions converge to the equilibrium set (LaSalle invariance principle).

% \subsection{Local (Allen--Cahn) limit as $\epsilon\to0$}
% Assume $m$ is smooth at the interaction scale. For normalized, radially symmetric kernels one has the classical expansion
% \begin{equation}\label{eq:conv_expansion}
%  (J_\epsilon*m)(x) \;=\; m(x) + c_2\,\epsilon^2\,\Delta m(x) + O(\epsilon^4),\quad c_2 = \frac{m_2}{2d},\; m_2=\int_{\mathbb{R}^d}|z|^2 J(z)\,dz.
% \end{equation}
% Linearizing $\tanh$ around $m+h$ gives
% \begin{equation}
%  \partial_t m \;\approx\; -m + \tanh\big(\beta(m+h)\big) + \beta\,c_2\,\epsilon^2\,\operatorname{sech}^2\!\big(\beta(m+h)\big)\,\Delta m,
% \end{equation}
% an Allen--Cahn type equation with state-dependent diffusion coefficient $\beta\,c_2\,\epsilon^2\,\operatorname{sech}^2(\beta(m+h))$.

% \subsection{Qualitative implications}
% Because $\widehat{J_\epsilon}(k)$ is maximal at $k=0$, the first instability is spatially uniform. Thus, beyond the mean-field threshold (e.g., $\beta>1$ at $h\approx0$), dynamics tends to select homogeneous magnetized states $\pm m^*$ rather than finite-wavenumber patterns; interfaces connecting phases may form, with their width and motion modulated by the nonlocal interaction range.


\nocite{*}
\printbibliography

\end{document}